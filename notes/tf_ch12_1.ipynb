{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke2AS4Wd119q"
   },
   "source": [
    "# Step45 - VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xhXQBuoZL7I",
    "outputId": "0b3e1a37-cfa6-4ea9-b209-1fe3d0bec2c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 02:20:37.877793: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 02:20:37.932717: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 02:20:39.134776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:20:39.150277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:20:39.150410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, GlobalMaxPool2D, Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "first = lambda x: x[0]\n",
    "second = lambda x: x[1]\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tllzOMTOgvUi"
   },
   "outputs": [],
   "source": [
    "\n",
    "class BenchmarkClassification:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.model = None\n",
    "        self.scaler = dict()\n",
    "        self.results = list()\n",
    "        self.scaler_type = None\n",
    "\n",
    "    def dataset(self, train_size=100):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def min_max_scaler(self, data, label='x'):\n",
    "        from collections import defaultdict\n",
    "        data_2d = np.reshape(data, (-1, 1)) if len(data.shape) ==1 else data\n",
    "        print('data_2d: ', data_2d.shape)\n",
    "        new_data = list()\n",
    "        for i, d in enumerate(data_2d.T):\n",
    "          scaler = MinMaxScaler()\n",
    "          #d = scaler.fit_transform(np.reshape(d, (-1, 1)))\n",
    "          new_data.append(scaler.fit_transform(np.reshape(d, (-1, 1))))\n",
    "          key = '{:}{:}'.format(label, i)\n",
    "          self.scaler[key] = scaler\n",
    "        #print(self.scaler)\n",
    "        print('new_data: ', np.squeeze(np.array(new_data), axis=2).T.shape)\n",
    "        return np.squeeze(np.array(new_data), axis=2).T\n",
    "\n",
    "    def standard_scaler(self, data, label='x'):\n",
    "        from collections import defaultdict\n",
    "        data_2d = np.reshape(data, (-1, 1)) if len(data.shape) ==1 else data\n",
    "        print('data_2d: ', data_2d.shape)\n",
    "        new_data = list()\n",
    "        for i, d in enumerate(data_2d.T):\n",
    "          scaler = StandardScaler()\n",
    "          #d = scaler.fit_transform(np.reshape(d, (-1, 1)))\n",
    "          new_data.append(scaler.fit_transform(np.reshape(d, (-1, 1))))\n",
    "          key = '{:}{:}'.format(label, i)\n",
    "          self.scaler[key] = scaler\n",
    "        #print(self.scaler)\n",
    "        print('new_data: ', np.squeeze(np.array(new_data), axis=2).T.shape)\n",
    "        return np.squeeze(np.array(new_data), axis=2).T\n",
    "\n",
    "    def inverse_transform(self, x, label='x'):\n",
    "        print(len(self.scaler), self.scaler)\n",
    "        x_2d = np.reshape(x, (-1, 1)) if len(x.shape) == 1 else x\n",
    "        #print('inverse - x_2d: ', x_2d.shape)\n",
    "        new_data = list()\n",
    "        for i, d in enumerate(x_2d.T):\n",
    "          key = '{:}{:}'.format(label, i)\n",
    "          #print('inverse - d:', d.shape, 'key:', key)\n",
    "          if key in self.scaler:\n",
    "            #print('d.reshape:', np.reshape(d, (-1, 1)).shape)\n",
    "            new_data.append(self.scaler[key].inverse_transform(np.reshape(d, (-1, 1))))\n",
    "        #print('inverse - new_data: ', np.squeeze(np.array(new_data), axis=2).T.shape)\n",
    "        return np.squeeze(np.array(new_data), axis=2).T\n",
    "\n",
    "    def make_hyper_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def make_optimizer(self, name, lr):\n",
    "        #print('make_optimizer', name)\n",
    "        if name == 'Adam':\n",
    "            return Adam(learning_rate=lr)\n",
    "        elif name == 'RMSprop':\n",
    "            return RMSprop(learning_rate=lr)\n",
    "        elif name == 'SGD':\n",
    "            return SGD(learning_rate=lr)\n",
    "        else:\n",
    "            raise ValueError('Unknown optimizer')\n",
    "\n",
    "    def make_loss_func(self, name):\n",
    "        if name == 'MAE':\n",
    "            return MeanAbsoluteError()\n",
    "        elif name == 'MSE':\n",
    "            return MeanSquaredError()\n",
    "        elif name == 'CCE':\n",
    "            return CategoricalCrossentropy()         \n",
    "        elif name == 'SCCE':\n",
    "            return SparseCategoricalCrossentropy()                     \n",
    "        else:\n",
    "            raise ValueError('Unknown loss function')\n",
    "\n",
    "    def make_model(self, opt='Adam', lr='0.01', loss_fn='MSE', **kargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, X, y, epochs=50, batch=1, callbacks=None, **kargs):\n",
    "        if self.model is not None:\n",
    "          if 'scaler' in kargs.keys():\n",
    "              self.scaler = dict()\n",
    "              X_train_scaled = self.apply_scaler(X, 'x', scaler_type=kargs['scaler'])\n",
    "              y_train_scaled = self.apply_scaler(y, 'y', scaler_type=kargs['scaler'])\n",
    "          else:\n",
    "              X_train_scaled = X\n",
    "              y_train_scaled = y\n",
    "          return self.model.fit(\n",
    "              X_train_scaled, \n",
    "              y_train_scaled, \n",
    "              epochs=epochs,\n",
    "              batch_size=batch, \n",
    "              callbacks=callbacks,\n",
    "              verbose=kargs['verbose'],\n",
    "              )\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        if self.scaler_type is None:\n",
    "          return self.model.evaluate(X_test, y_test)\n",
    "        else:\n",
    "          X_scaled = self.apply_scaler(X, 'x', self.scaler_type)\n",
    "          y_true_scaled = self.apply_scaler(y_true, 'y', self.scaler_type)\n",
    "          return self.model.evaluate(X_scaled, y_true_scaled)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def get_result(self):\n",
    "        return self.results[-1]\n",
    "\n",
    "    def plot_result(self, ylabels=['loss'], xlabel='epochs'):\n",
    "        parsed_results = self.get_result()\n",
    "        #print('plot_result', parsed_results)\n",
    "        n_plots = len(ylabels)\n",
    "        fig, ax = plt.subplots(1, n_plots, figsize=(5*n_plots, 5))\n",
    "        for i, ylabel in enumerate(ylabels):\n",
    "          ax[i].plot(parsed_results[ylabel])\n",
    "          ax[i].set_xlabel(xlabel)\n",
    "          ax[i].set_ylabel(ylabel)\n",
    "        plt.suptitle('The Best Performance Model for {}'.format(self.name))\n",
    "        plt.show()\n",
    "\n",
    "    def plot_predict_scaled(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        x_origin = self.inverse_transform(X, 'x')\n",
    "        y_origin = self.inverse_transform(y_true, 'y')\n",
    "        y_pred_origin = self.inverse_transform(y_pred, 'y')\n",
    "        #plt.scatter(x_origin, y_origin)\n",
    "        plt.plot(y_origin, label='True', color='blue')\n",
    "        plt.plot(y_pred_origin, label='Predicted', color='red')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_predict_unscaled(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        print('X: {}, y: {}, y_pred: {}'.format(X.shape, y_true.shape, y_pred.shape))\n",
    "        #plt.scatter(X, y_true)\n",
    "        plt.plot(y_true, label='True', color='blue')\n",
    "        plt.plot(y_pred, label='Predicted', color='red')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_predict(self, X, y_true, scaled=False):\n",
    "        print('plot_predict', self.scaler_type)\n",
    "        if self.scaler_type is None:\n",
    "          self.plot_predict_unscaled(X, y_true)\n",
    "        else:\n",
    "          if not scaled:\n",
    "            X_scaled = self.apply_scaler(X, 'x', self.scaler_type)\n",
    "            y_true_scaled = self.apply_scaler(y_true, 'y', self.scaler_type)\n",
    "          self.plot_predict_scaled(X_scaled, y_true_scaled)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "    \n",
    "    def apply_scaler(self, data, label='x', scaler_type='Standard'):\n",
    "        print('apply_scaler: ', data.shape, label, scaler_type)\n",
    "        self.scaler_type = scaler_type\n",
    "        if self.scaler_type == 'Standard':\n",
    "          return self.standard_scaler(data, label)\n",
    "        elif self.scaler_type == 'MinMax':\n",
    "          return self.min_max_scaler(data, label)\n",
    "\n",
    "    def save_best_model(self):\n",
    "        self.best_model = self.model\n",
    "        self.best_scaler = self.scaler\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self, model_path='./model'):\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def save_model_weights(self, checkpoint_path='./cp/cp-{epoch:04d}.ckpt'):\n",
    "        return self.model.save_weights(checkpoint_path)\n",
    "\n",
    "    def load_best_model(self):\n",
    "        self.model = self.best_model\n",
    "        self.scaler = self.best_scaler\n",
    "        self.load_model()\n",
    "        print(self.model.summary())\n",
    "        print(self.best_param)\n",
    "        print(self.best_scaler_type)\n",
    "        print(self.scaler)\n",
    "\n",
    "    def load_model(self, model_path='./model'):\n",
    "        return load_model(model_path)\n",
    "\n",
    "    def load_model_weights(self, checkpoint_path='./cp/cp-{epoch:04d}.ckpt'):\n",
    "        return self.model.load_weights(checkpoint_path)\n",
    "\n",
    "    def load_lastest_checkpoint(self, checkpoint_path='./cp'):\n",
    "        latest = tf.keras.models.lastest_checkpoint(checkpoint_path)\n",
    "        return self.model.load_weights(latest)\n",
    "\n",
    "    def make_checkpoint_callback(self, save_best_only=True, period=5):\n",
    "        filepath = \"./cp/cp-{epoch:04d}.ckpt\"\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath, \n",
    "            save_best_only=save_best_only,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss',\n",
    "            mode='min', \n",
    "            verbose=1, \n",
    "            save_freq='epoch', \n",
    "            period=period,\n",
    "            )\n",
    "        return checkpoint\n",
    "\n",
    "    def make_tensorboard_callback(self, log_dir='./logs'):\n",
    "        tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_images=True,\n",
    "                                  update_freq='epoch', profile_batch=2, embeddings_freq=1)\n",
    "        return tensorboard\n",
    "\n",
    "    # TODO: how can we selelct the best model from the benchmark\n",
    "    def benchmark(self, X, y, params=None):\n",
    "        import sys\n",
    "        X_train, X_test, y_train, y_test =\\\n",
    "         train_test_split(\n",
    "             X, \n",
    "             y, \n",
    "             test_size=0.2, \n",
    "             random_state=42,\n",
    "             )\n",
    "        hyper_params = self.make_hyper_params() if params is None else params\n",
    "        min_loss = sys.float_info.max\n",
    "        for i, param in enumerate(hyper_params):\n",
    "          print('*'*20)\n",
    "          print('#{}, opt: {}, lr: {}, loss_fn: {}, batch: {}'\\\n",
    "                .format(i, param['opt'], param['lr'], param['loss_fn'], param['batch']))\n",
    "          self.make_model(**param)\n",
    "          record = self.train(X, y, **param)\n",
    "          result = self.parse_result(record)\n",
    "          print('loss', result['loss'])\n",
    "          if result['loss'][-1] < min_loss:\n",
    "            #print('loss: {:.2f}, weights: {}, bias: {}'\\\n",
    "            #      .format(result['loss'][-1], result['weights'], result['bias']))          \n",
    "            score = self.evaluate(X_test, y_test)\n",
    "            result['score'] = score\n",
    "            self.results.append(result)\n",
    "            self.best_param = param\n",
    "            if 'scaler' in param.keys():\n",
    "              self.best_scaler_type = param['scaler']\n",
    "            self.save_best_model()\n",
    "        self.load_best_model()\n",
    "        return self.results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd3TpEYn32_7"
   },
   "source": [
    "## 16K CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5ZGXh-Ai1m_"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Cifar10Benchmark(BenchmarkClassification):\n",
    "    def __init__(self, name='Cifar', train_size=600):\n",
    "        super().__init__(name)\n",
    "        #self.X, self.y = self.dataset(train_size)\n",
    "\n",
    "    def dataset(self, shuffle=True, train_size=100):\n",
    "          train, test = cifar10.load_data()\n",
    "          self.input_shape = first(train).shape[1:]\n",
    "          print('self.input_shape', self.input_shape)\n",
    "          return train, test\n",
    "\n",
    "    def min_max_scaler(self, data, label='x'):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        scaled_data = scaler.fit_transform(data)\n",
    "        print(scaled_data.shape)\n",
    "        return scaled_data\n",
    "\n",
    "    def inverse_transform(self, x, label='x'):\n",
    "        inversed_data = self.scaler.inverse_transform(x)\n",
    "        print('inversed_data: ', x.shape)\n",
    "        return inversed_data\n",
    "\n",
    "    def parse_result(self, result):\n",
    "        print('parse_result: ', result.history.keys())\n",
    "        parsed_result = {\n",
    "            'loss': result.history['loss'],\n",
    "            'weights': np.array([]),\n",
    "            'bias': np.array([])\n",
    "        }\n",
    "        if 'metrics' in self.params:\n",
    "          for m in self.params['metrics']:\n",
    "            if m in result.history:\n",
    "              parsed_result[m] = result.history[m]\n",
    "        print('parsed_result: ', parsed_result.keys())\n",
    "        return parsed_result\n",
    "\n",
    "    def make_hyper_params(self):\n",
    "          self.params = {\n",
    "              'opt': ['Adam', 'RMSprop'],\n",
    "              'lr': [0.01, 0.001],\n",
    "              'loss_fn': ['CCE'],\n",
    "              'metrics': ['accuracy'],\n",
    "              'batch': [400, 800],\n",
    "              'validation_split': [0.3],\n",
    "              'verbose': [2],\n",
    "              'scaler': ['MinMax'],\n",
    "          }\n",
    "          import itertools\n",
    "          permutations_dicts = [dict(zip(self.params.keys(), v))  for v in itertools.product(*self.params.values())]\n",
    "          print(permutations_dicts)\n",
    "          return permutations_dicts\n",
    "\n",
    "    def make_model(self, opt='Adam', lr='0.01', loss_fn='MSE', **kargs):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(Input(self.input_shape)) # shape=(32, 32, 3)\n",
    "        self.model.add(Conv2D(filters=16, kernel_size = (3,3), activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPool2D())\n",
    "\n",
    "        self.model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
    "        self.model.add(MaxPool2D())\n",
    "        self.model.add(Dropout( rate=0.2))\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dropout(rate=0.5))\n",
    "        self.model.add(Dense(units=10, activation='softmax'))\n",
    "        self.model.summary()\n",
    "\n",
    "        optimizer = self.make_optimizer(opt, lr)\n",
    "        loss_func = self.make_loss_func(loss_fn)\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_func, metrics=kargs['metrics'])\n",
    "        return self.model\n",
    "\n",
    "    def train(self, X, y, epochs=50, batch=1, callbacks=None, **kargs):\n",
    "        if self.model is not None:\n",
    "          if 'scaler' in kargs.keys() and self.scaler is None:\n",
    "              X_train_scaled = self.apply_scaler(data)\n",
    "          else:\n",
    "              X_train_scaled = X\n",
    "          categorical_y = tf.keras.utils.to_categorical(y)\n",
    "\n",
    "          return self.model.fit(\n",
    "              X_train_scaled, \n",
    "              categorical_y, \n",
    "              epochs=epochs,\n",
    "              batch_size=batch, \n",
    "              callbacks=callbacks,\n",
    "              verbose=kargs['verbose'],\n",
    "              validation_split=kargs['validation_split'],\n",
    "              )\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        if self.model is not None:\n",
    "            if self.scaler is None:\n",
    "                X_scaled = self.apply_scaler(data)\n",
    "            else:\n",
    "                X_scaled = X\n",
    "            categorical_y = tf.keras.utils.to_categorical(y_true)\n",
    "            \n",
    "            return self.model.evaluate(X_scaled, categorical_y)\n",
    "        # if self.scaler_type is None:\n",
    "        #   return self.model.evaluate(X_test, y_test)\n",
    "        # else:\n",
    "        #   X_scaled = self.apply_scaler(X, 'x', self.scaler_type)\n",
    "        #   y_true_scaled = self.apply_scaler(y_true, 'y', self.scaler_type)\n",
    "          \n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.model is not None:\n",
    "              if self.scaler is None:\n",
    "                  X_scaled = self.apply_scaler(data)\n",
    "              else:\n",
    "                  X_scaled = X\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def benchmark(self, X, y, X_test, y_test, params=None):\n",
    "        import sys\n",
    "        X_train, X_test, y_train, y_test =\\\n",
    "         train_test_split(\n",
    "             X, \n",
    "             y, \n",
    "             test_size=0.2, \n",
    "             random_state=42,\n",
    "             )\n",
    "        hyper_params = self.make_hyper_params() if params is None else params\n",
    "        min_loss = sys.float_info.max\n",
    "        for i, param in enumerate(hyper_params):\n",
    "          print('*'*20)\n",
    "          print('#{}, opt: {}, lr: {}, loss_fn: {}, batch: {}'\\\n",
    "                .format(i, param['opt'], param['lr'], param['loss_fn'], param['batch']))\n",
    "          self.make_model(**param)\n",
    "          param.update({\n",
    "              'callbacks': [self.make_checkpoint_callback(period=0),\n",
    "              self.make_tensorboard_callback()],\n",
    "              })\n",
    "          record = self.train(X, y, **param)\n",
    "          result = self.parse_result(record)\n",
    "          print('loss', result['loss'])\n",
    "          if result['loss'][-1] < min_loss:      \n",
    "            score = self.evaluate(X_test, y_test)\n",
    "            result['score'] = score\n",
    "            self.results.append(result)\n",
    "            self.best_param = param\n",
    "            if 'scaler' in param.keys():\n",
    "              self.best_scaler_type = param['scaler']\n",
    "            self.save_best_model()\n",
    "        self.load_best_model()\n",
    "        return self.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OskzFB1gLxDJ",
    "outputId": "a8fda503-76b7-4a23-fff7-d7a092ba1dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.input_shape (32, 32, 3)\n",
      "X: (50000, 32, 32, 3), y: (50000, 1)\n",
      "X: (10000, 32, 32, 3), y: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "benchmark = Cifar10Benchmark()\n",
    "train, test = benchmark.dataset()\n",
    "X_train  = first(train)\n",
    "y_train  = second(train)\n",
    "X_test  = first(test)\n",
    "y_test  = second(test)\n",
    "print('X: {}, y: {}'.format(X_train.shape, y_train.shape))\n",
    "print('X: {}, y: {}'.format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8tHk6-DFL-u-",
    "outputId": "05269814-de6a-4106-ebe6-394649fcc49e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'opt': 'Adam', 'lr': 0.01, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 400, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}, {'opt': 'Adam', 'lr': 0.01, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}, {'opt': 'Adam', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 400, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}, {'opt': 'Adam', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}, {'opt': 'RMSprop', 'lr': 0.01, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 400, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}, {'opt': 'RMSprop', 'lr': 0.01, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}, {'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 400, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}, {'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}]\n",
      "********************\n",
      "#0, opt: Adam, lr: 0.01, loss_fn: CCE, batch: 400\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 30, 30, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:46:09.780472: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:46:09.780491: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:46:09.942747: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:46:09.945593: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:46:10.342128: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_7/dropout_10/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:46:10.706023: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:46:10.706041: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:46:10.880835: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:46:10.883541: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:46:10.889473: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 123 callback api events and 124 activity events. \n",
      "2023-05-07 12:46:10.890447: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:46:10.890561: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_46_10/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.69192, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 2s - loss: 1.9615 - accuracy: 0.3245 - val_loss: 1.6919 - val_accuracy: 0.4159 - 2s/epoch - 23ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 1.69192 to 1.49797, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.5596 - accuracy: 0.4412 - val_loss: 1.4980 - val_accuracy: 0.4669 - 980ms/epoch - 11ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 1.49797 to 1.48757, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.4531 - accuracy: 0.4831 - val_loss: 1.4876 - val_accuracy: 0.4501 - 985ms/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 1.48757 to 1.34689, saving model to ./cp/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.3922 - accuracy: 0.5086 - val_loss: 1.3469 - val_accuracy: 0.5246 - 1s/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.34689 to 1.21465, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.3325 - accuracy: 0.5267 - val_loss: 1.2147 - val_accuracy: 0.5817 - 957ms/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.21465\n",
      "88/88 - 1s - loss: 1.2917 - accuracy: 0.5475 - val_loss: 1.2230 - val_accuracy: 0.5737 - 606ms/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss improved from 1.21465 to 1.19258, saving model to ./cp/cp-0007.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2563 - accuracy: 0.5578 - val_loss: 1.1926 - val_accuracy: 0.5959 - 953ms/epoch - 11ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss improved from 1.19258 to 1.17735, saving model to ./cp/cp-0008.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2322 - accuracy: 0.5680 - val_loss: 1.1774 - val_accuracy: 0.5841 - 1s/epoch - 13ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.17735\n",
      "88/88 - 1s - loss: 1.2175 - accuracy: 0.5741 - val_loss: 1.3416 - val_accuracy: 0.5495 - 605ms/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss improved from 1.17735 to 1.13203, saving model to ./cp/cp-0010.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0010.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0010.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1979 - accuracy: 0.5834 - val_loss: 1.1320 - val_accuracy: 0.6033 - 1s/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.13203\n",
      "88/88 - 1s - loss: 1.1771 - accuracy: 0.5884 - val_loss: 1.2014 - val_accuracy: 0.5785 - 582ms/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.13203\n",
      "88/88 - 1s - loss: 1.1641 - accuracy: 0.5944 - val_loss: 1.1698 - val_accuracy: 0.5985 - 580ms/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.13203\n",
      "88/88 - 1s - loss: 1.1651 - accuracy: 0.5936 - val_loss: 1.1489 - val_accuracy: 0.6101 - 589ms/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss improved from 1.13203 to 1.10650, saving model to ./cp/cp-0014.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1444 - accuracy: 0.6039 - val_loss: 1.1065 - val_accuracy: 0.6186 - 953ms/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss improved from 1.10650 to 1.02832, saving model to ./cp/cp-0015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1308 - accuracy: 0.6096 - val_loss: 1.0283 - val_accuracy: 0.6522 - 951ms/epoch - 11ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.02832\n",
      "88/88 - 1s - loss: 1.1292 - accuracy: 0.6079 - val_loss: 1.0641 - val_accuracy: 0.6399 - 611ms/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.02832\n",
      "88/88 - 1s - loss: 1.1160 - accuracy: 0.6129 - val_loss: 1.1842 - val_accuracy: 0.5945 - 622ms/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.02832\n",
      "88/88 - 1s - loss: 1.1209 - accuracy: 0.6099 - val_loss: 1.0922 - val_accuracy: 0.6240 - 588ms/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss improved from 1.02832 to 1.00548, saving model to ./cp/cp-0019.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1123 - accuracy: 0.6129 - val_loss: 1.0055 - val_accuracy: 0.6601 - 957ms/epoch - 11ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.00548\n",
      "88/88 - 1s - loss: 1.1000 - accuracy: 0.6198 - val_loss: 1.2040 - val_accuracy: 0.5850 - 586ms/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.00548\n",
      "88/88 - 1s - loss: 1.0987 - accuracy: 0.6171 - val_loss: 1.0136 - val_accuracy: 0.6629 - 572ms/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss improved from 1.00548 to 1.00022, saving model to ./cp/cp-0022.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0888 - accuracy: 0.6221 - val_loss: 1.0002 - val_accuracy: 0.6572 - 1s/epoch - 13ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.00022\n",
      "88/88 - 1s - loss: 1.0907 - accuracy: 0.6205 - val_loss: 1.0261 - val_accuracy: 0.6529 - 644ms/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss improved from 1.00022 to 0.99487, saving model to ./cp/cp-0024.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0024.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0024.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0948 - accuracy: 0.6195 - val_loss: 0.9949 - val_accuracy: 0.6634 - 977ms/epoch - 11ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.99487\n",
      "88/88 - 1s - loss: 1.0840 - accuracy: 0.6215 - val_loss: 1.0904 - val_accuracy: 0.6160 - 584ms/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.99487\n",
      "88/88 - 1s - loss: 1.0838 - accuracy: 0.6208 - val_loss: 1.0168 - val_accuracy: 0.6557 - 572ms/epoch - 6ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.99487\n",
      "88/88 - 1s - loss: 1.0753 - accuracy: 0.6279 - val_loss: 1.0345 - val_accuracy: 0.6578 - 590ms/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss improved from 0.99487 to 0.98581, saving model to ./cp/cp-0028.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0673 - accuracy: 0.6309 - val_loss: 0.9858 - val_accuracy: 0.6596 - 963ms/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.98581\n",
      "88/88 - 1s - loss: 1.0676 - accuracy: 0.6271 - val_loss: 1.0193 - val_accuracy: 0.6483 - 577ms/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss improved from 0.98581 to 0.97336, saving model to ./cp/cp-0030.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0838 - accuracy: 0.6241 - val_loss: 0.9734 - val_accuracy: 0.6663 - 978ms/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0732 - accuracy: 0.6280 - val_loss: 1.0005 - val_accuracy: 0.6591 - 644ms/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0758 - accuracy: 0.6265 - val_loss: 1.0449 - val_accuracy: 0.6359 - 598ms/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0618 - accuracy: 0.6329 - val_loss: 1.0104 - val_accuracy: 0.6537 - 609ms/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0654 - accuracy: 0.6295 - val_loss: 0.9860 - val_accuracy: 0.6628 - 627ms/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0562 - accuracy: 0.6318 - val_loss: 0.9881 - val_accuracy: 0.6638 - 639ms/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0542 - accuracy: 0.6345 - val_loss: 1.0291 - val_accuracy: 0.6419 - 617ms/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0668 - accuracy: 0.6291 - val_loss: 1.0917 - val_accuracy: 0.6135 - 581ms/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0610 - accuracy: 0.6327 - val_loss: 0.9760 - val_accuracy: 0.6690 - 574ms/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.97336\n",
      "88/88 - 1s - loss: 1.0546 - accuracy: 0.6348 - val_loss: 0.9850 - val_accuracy: 0.6615 - 665ms/epoch - 8ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss improved from 0.97336 to 0.96190, saving model to ./cp/cp-0040.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0040.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0040.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0479 - accuracy: 0.6373 - val_loss: 0.9619 - val_accuracy: 0.6663 - 1s/epoch - 12ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.96190\n",
      "88/88 - 1s - loss: 1.0450 - accuracy: 0.6412 - val_loss: 1.0028 - val_accuracy: 0.6575 - 608ms/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.96190\n",
      "88/88 - 1s - loss: 1.0490 - accuracy: 0.6373 - val_loss: 0.9897 - val_accuracy: 0.6643 - 592ms/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.96190\n",
      "88/88 - 1s - loss: 1.0530 - accuracy: 0.6349 - val_loss: 1.0103 - val_accuracy: 0.6502 - 590ms/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.96190\n",
      "88/88 - 1s - loss: 1.0367 - accuracy: 0.6433 - val_loss: 1.0185 - val_accuracy: 0.6461 - 629ms/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.96190\n",
      "88/88 - 1s - loss: 1.0464 - accuracy: 0.6376 - val_loss: 1.0309 - val_accuracy: 0.6581 - 581ms/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.96190\n",
      "88/88 - 1s - loss: 1.0428 - accuracy: 0.6390 - val_loss: 1.0099 - val_accuracy: 0.6573 - 583ms/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.96190\n",
      "88/88 - 1s - loss: 1.0490 - accuracy: 0.6378 - val_loss: 1.0603 - val_accuracy: 0.6383 - 602ms/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss improved from 0.96190 to 0.93031, saving model to ./cp/cp-0048.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0048.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0048.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0437 - accuracy: 0.6406 - val_loss: 0.9303 - val_accuracy: 0.6866 - 1s/epoch - 11ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.93031\n",
      "88/88 - 1s - loss: 1.0453 - accuracy: 0.6352 - val_loss: 0.9954 - val_accuracy: 0.6599 - 633ms/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.93031\n",
      "88/88 - 1s - loss: 1.0458 - accuracy: 0.6373 - val_loss: 0.9847 - val_accuracy: 0.6647 - 598ms/epoch - 7ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [1.961464285850525, 1.559567928314209, 1.453139066696167, 1.3922263383865356, 1.3325080871582031, 1.2917020320892334, 1.2563267946243286, 1.232177734375, 1.217539668083191, 1.1978613138198853, 1.1771020889282227, 1.1640703678131104, 1.1650731563568115, 1.1443865299224854, 1.1308428049087524, 1.1292035579681396, 1.115991234779358, 1.1208680868148804, 1.1123278141021729, 1.1000123023986816, 1.0986922979354858, 1.0887953042984009, 1.0906941890716553, 1.094831943511963, 1.0839585065841675, 1.0838046073913574, 1.0752500295639038, 1.0673338174819946, 1.0675654411315918, 1.083829402923584, 1.0731635093688965, 1.0758174657821655, 1.0617694854736328, 1.0654460191726685, 1.056167721748352, 1.0541852712631226, 1.066815972328186, 1.0609787702560425, 1.0545718669891357, 1.0479127168655396, 1.0449939966201782, 1.0489921569824219, 1.0529593229293823, 1.0367162227630615, 1.0464071035385132, 1.042804479598999, 1.0490292310714722, 1.0437120199203491, 1.0453389883041382, 1.0458152294158936]\n",
      "313/313 [==============================] - 0s 993us/step - loss: 0.8864 - accuracy: 0.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#1, opt: Adam, lr: 0.01, loss_fn: CCE, batch: 800\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 30, 30, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:46:49.027706: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:46:49.027723: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:46:49.122155: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:46:49.127192: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:46:49.527486: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_8/dropout_12/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:46:50.200386: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:46:50.200405: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:46:50.310757: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:46:50.314285: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:46:50.320906: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 122 callback api events and 123 activity events. \n",
      "2023-05-07 12:46:50.321934: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:46:50.322057: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_46_50/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.84459, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 2.1676 - accuracy: 0.2715 - val_loss: 2.8446 - val_accuracy: 0.2729 - 2s/epoch - 52ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 2.84459 to 1.65406, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.5935 - accuracy: 0.4285 - val_loss: 1.6541 - val_accuracy: 0.4475 - 901ms/epoch - 20ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.65406\n",
      "44/44 - 1s - loss: 1.4488 - accuracy: 0.4814 - val_loss: 1.9700 - val_accuracy: 0.4049 - 524ms/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 1.65406 to 1.58583, saving model to ./cp/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3876 - accuracy: 0.5059 - val_loss: 1.5858 - val_accuracy: 0.4775 - 914ms/epoch - 21ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.58583 to 1.57100, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3289 - accuracy: 0.5299 - val_loss: 1.5710 - val_accuracy: 0.4993 - 1s/epoch - 24ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss improved from 1.57100 to 1.33100, saving model to ./cp/cp-0006.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2914 - accuracy: 0.5478 - val_loss: 1.3310 - val_accuracy: 0.5447 - 926ms/epoch - 21ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.33100\n",
      "44/44 - 1s - loss: 1.2752 - accuracy: 0.5519 - val_loss: 1.3345 - val_accuracy: 0.5343 - 542ms/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.33100\n",
      "44/44 - 1s - loss: 1.2419 - accuracy: 0.5660 - val_loss: 1.3939 - val_accuracy: 0.4939 - 539ms/epoch - 12ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss improved from 1.33100 to 1.19738, saving model to ./cp/cp-0009.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2033 - accuracy: 0.5734 - val_loss: 1.1974 - val_accuracy: 0.5911 - 917ms/epoch - 21ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.19738\n",
      "44/44 - 1s - loss: 1.1779 - accuracy: 0.5862 - val_loss: 1.2266 - val_accuracy: 0.5794 - 534ms/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss improved from 1.19738 to 1.17263, saving model to ./cp/cp-0011.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1871 - accuracy: 0.5856 - val_loss: 1.1726 - val_accuracy: 0.5945 - 948ms/epoch - 22ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.17263\n",
      "44/44 - 1s - loss: 1.1473 - accuracy: 0.6009 - val_loss: 1.1865 - val_accuracy: 0.5841 - 542ms/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss improved from 1.17263 to 1.13305, saving model to ./cp/cp-0013.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1574 - accuracy: 0.5961 - val_loss: 1.1330 - val_accuracy: 0.6225 - 897ms/epoch - 20ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.13305\n",
      "44/44 - 1s - loss: 1.1217 - accuracy: 0.6087 - val_loss: 1.1478 - val_accuracy: 0.5988 - 529ms/epoch - 12ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.13305\n",
      "44/44 - 1s - loss: 1.1160 - accuracy: 0.6143 - val_loss: 1.2005 - val_accuracy: 0.5779 - 553ms/epoch - 13ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss improved from 1.13305 to 1.08473, saving model to ./cp/cp-0016.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1166 - accuracy: 0.6100 - val_loss: 1.0847 - val_accuracy: 0.6281 - 898ms/epoch - 20ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss improved from 1.08473 to 1.07018, saving model to ./cp/cp-0017.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0017.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0017.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0973 - accuracy: 0.6157 - val_loss: 1.0702 - val_accuracy: 0.6313 - 1s/epoch - 24ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss improved from 1.07018 to 0.98927, saving model to ./cp/cp-0018.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0018.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0018.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0720 - accuracy: 0.6287 - val_loss: 0.9893 - val_accuracy: 0.6589 - 904ms/epoch - 21ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.98927\n",
      "44/44 - 1s - loss: 1.0776 - accuracy: 0.6273 - val_loss: 1.0811 - val_accuracy: 0.6205 - 537ms/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.98927\n",
      "44/44 - 1s - loss: 1.0876 - accuracy: 0.6223 - val_loss: 1.0682 - val_accuracy: 0.6345 - 546ms/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.98927\n",
      "44/44 - 1s - loss: 1.0778 - accuracy: 0.6242 - val_loss: 0.9900 - val_accuracy: 0.6583 - 541ms/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.98927\n",
      "44/44 - 1s - loss: 1.0676 - accuracy: 0.6307 - val_loss: 1.4277 - val_accuracy: 0.5170 - 531ms/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss improved from 0.98927 to 0.97866, saving model to ./cp/cp-0023.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0504 - accuracy: 0.6342 - val_loss: 0.9787 - val_accuracy: 0.6614 - 914ms/epoch - 21ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.97866\n",
      "44/44 - 1s - loss: 1.0602 - accuracy: 0.6337 - val_loss: 1.0465 - val_accuracy: 0.6397 - 533ms/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.97866\n",
      "44/44 - 1s - loss: 1.0540 - accuracy: 0.6375 - val_loss: 1.0106 - val_accuracy: 0.6504 - 536ms/epoch - 12ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.97866\n",
      "44/44 - 1s - loss: 1.0478 - accuracy: 0.6369 - val_loss: 1.0046 - val_accuracy: 0.6538 - 531ms/epoch - 12ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss improved from 0.97866 to 0.97822, saving model to ./cp/cp-0027.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0027.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0027.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0366 - accuracy: 0.6397 - val_loss: 0.9782 - val_accuracy: 0.6700 - 923ms/epoch - 21ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.97822\n",
      "44/44 - 1s - loss: 1.0369 - accuracy: 0.6414 - val_loss: 1.0278 - val_accuracy: 0.6417 - 533ms/epoch - 12ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.97822\n",
      "44/44 - 1s - loss: 1.0367 - accuracy: 0.6401 - val_loss: 1.0963 - val_accuracy: 0.6267 - 525ms/epoch - 12ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.97822\n",
      "44/44 - 1s - loss: 1.0324 - accuracy: 0.6421 - val_loss: 1.0059 - val_accuracy: 0.6477 - 536ms/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.97822\n",
      "44/44 - 1s - loss: 1.0248 - accuracy: 0.6451 - val_loss: 0.9973 - val_accuracy: 0.6585 - 544ms/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.97822\n",
      "44/44 - 1s - loss: 1.0284 - accuracy: 0.6437 - val_loss: 1.0052 - val_accuracy: 0.6514 - 527ms/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss improved from 0.97822 to 0.94446, saving model to ./cp/cp-0033.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0033.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0033.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0289 - accuracy: 0.6413 - val_loss: 0.9445 - val_accuracy: 0.6772 - 897ms/epoch - 20ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.94446\n",
      "44/44 - 1s - loss: 1.0174 - accuracy: 0.6473 - val_loss: 1.0164 - val_accuracy: 0.6417 - 527ms/epoch - 12ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.94446\n",
      "44/44 - 1s - loss: 1.0079 - accuracy: 0.6531 - val_loss: 0.9876 - val_accuracy: 0.6571 - 544ms/epoch - 12ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.94446\n",
      "44/44 - 1s - loss: 1.0111 - accuracy: 0.6479 - val_loss: 1.0197 - val_accuracy: 0.6445 - 540ms/epoch - 12ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.94446\n",
      "44/44 - 1s - loss: 1.0092 - accuracy: 0.6505 - val_loss: 0.9484 - val_accuracy: 0.6763 - 532ms/epoch - 12ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.94446\n",
      "44/44 - 1s - loss: 1.0011 - accuracy: 0.6522 - val_loss: 1.0648 - val_accuracy: 0.6352 - 532ms/epoch - 12ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.94446\n",
      "44/44 - 1s - loss: 1.0079 - accuracy: 0.6503 - val_loss: 1.0771 - val_accuracy: 0.6354 - 530ms/epoch - 12ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss improved from 0.94446 to 0.92202, saving model to ./cp/cp-0040.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0040.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0040.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 0.9991 - accuracy: 0.6527 - val_loss: 0.9220 - val_accuracy: 0.6907 - 1s/epoch - 24ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 1.0130 - accuracy: 0.6475 - val_loss: 1.0085 - val_accuracy: 0.6511 - 552ms/epoch - 13ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 1.0070 - accuracy: 0.6532 - val_loss: 0.9232 - val_accuracy: 0.6900 - 544ms/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 1.0032 - accuracy: 0.6548 - val_loss: 0.9890 - val_accuracy: 0.6675 - 528ms/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 0.9944 - accuracy: 0.6558 - val_loss: 0.9676 - val_accuracy: 0.6659 - 526ms/epoch - 12ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 0.9897 - accuracy: 0.6570 - val_loss: 1.0208 - val_accuracy: 0.6430 - 546ms/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 1.0004 - accuracy: 0.6544 - val_loss: 0.9405 - val_accuracy: 0.6759 - 526ms/epoch - 12ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 0.9901 - accuracy: 0.6575 - val_loss: 0.9574 - val_accuracy: 0.6776 - 542ms/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 0.9908 - accuracy: 0.6525 - val_loss: 1.0257 - val_accuracy: 0.6511 - 535ms/epoch - 12ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 0.9997 - accuracy: 0.6549 - val_loss: 0.9293 - val_accuracy: 0.6801 - 535ms/epoch - 12ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.92202\n",
      "44/44 - 1s - loss: 0.9842 - accuracy: 0.6600 - val_loss: 1.2479 - val_accuracy: 0.5897 - 541ms/epoch - 12ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.167642116546631, 1.5935486555099487, 1.448815107345581, 1.3875725269317627, 1.3289028406143188, 1.2914389371871948, 1.2751970291137695, 1.2418869733810425, 1.203323483467102, 1.1779329776763916, 1.1870543956756592, 1.147301197052002, 1.1573783159255981, 1.1217126846313477, 1.115986704826355, 1.1165610551834106, 1.0973267555236816, 1.072024941444397, 1.077648401260376, 1.0875663757324219, 1.0777918100357056, 1.067570447921753, 1.0504428148269653, 1.0602164268493652, 1.0539501905441284, 1.0477943420410156, 1.0366147756576538, 1.0368821620941162, 1.0367157459259033, 1.0324194431304932, 1.0247644186019897, 1.028439998626709, 1.0288560390472412, 1.0173723697662354, 1.0079467296600342, 1.0111442804336548, 1.009183645248413, 1.0010859966278076, 1.007885456085205, 0.9990599155426025, 1.013015866279602, 1.006996989250183, 1.0032250881195068, 0.9944204092025757, 0.9896648526191711, 1.0003650188446045, 0.9901118278503418, 0.9908434152603149, 0.9997311234474182, 0.9842401742935181]\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.1589 - accuracy: 0.6142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#2, opt: Adam, lr: 0.001, loss_fn: CCE, batch: 400\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 30, 30, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:47:24.513741: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:47:24.513762: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:47:24.609575: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:47:24.614688: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:47:25.013768: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_9/dropout_14/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:47:25.378038: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:47:25.378058: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:47:25.485822: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:47:25.489595: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:47:25.496315: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 123 callback api events and 124 activity events. \n",
      "2023-05-07 12:47:25.497472: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:47:25.497610: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_47_25/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.94870, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 2s - loss: 2.2028 - accuracy: 0.2506 - val_loss: 1.9487 - val_accuracy: 0.3205 - 2s/epoch - 21ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 1.94870 to 1.58808, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.6977 - accuracy: 0.3803 - val_loss: 1.5881 - val_accuracy: 0.4329 - 1s/epoch - 12ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 1.58808 to 1.56954, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.5538 - accuracy: 0.4361 - val_loss: 1.5695 - val_accuracy: 0.4353 - 945ms/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 1.56954 to 1.50033, saving model to ./cp/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.4689 - accuracy: 0.4723 - val_loss: 1.5003 - val_accuracy: 0.4613 - 953ms/epoch - 11ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.50033\n",
      "88/88 - 1s - loss: 1.4004 - accuracy: 0.5035 - val_loss: 1.6286 - val_accuracy: 0.4535 - 593ms/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.50033\n",
      "88/88 - 1s - loss: 1.3328 - accuracy: 0.5275 - val_loss: 1.6179 - val_accuracy: 0.4533 - 584ms/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss improved from 1.50033 to 1.48080, saving model to ./cp/cp-0007.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2868 - accuracy: 0.5451 - val_loss: 1.4808 - val_accuracy: 0.4927 - 1s/epoch - 13ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss improved from 1.48080 to 1.36696, saving model to ./cp/cp-0008.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2489 - accuracy: 0.5610 - val_loss: 1.3670 - val_accuracy: 0.5079 - 965ms/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss improved from 1.36696 to 1.17297, saving model to ./cp/cp-0009.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2218 - accuracy: 0.5699 - val_loss: 1.1730 - val_accuracy: 0.5893 - 947ms/epoch - 11ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.17297\n",
      "88/88 - 1s - loss: 1.1872 - accuracy: 0.5831 - val_loss: 1.3195 - val_accuracy: 0.5416 - 575ms/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss improved from 1.17297 to 1.13849, saving model to ./cp/cp-0011.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1712 - accuracy: 0.5901 - val_loss: 1.1385 - val_accuracy: 0.5970 - 980ms/epoch - 11ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss improved from 1.13849 to 1.09994, saving model to ./cp/cp-0012.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0012.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0012.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1533 - accuracy: 0.5928 - val_loss: 1.0999 - val_accuracy: 0.6180 - 990ms/epoch - 11ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.09994\n",
      "88/88 - 1s - loss: 1.1398 - accuracy: 0.6037 - val_loss: 1.1620 - val_accuracy: 0.5911 - 631ms/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.09994\n",
      "88/88 - 1s - loss: 1.1270 - accuracy: 0.6070 - val_loss: 1.2255 - val_accuracy: 0.5789 - 606ms/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss improved from 1.09994 to 1.06588, saving model to ./cp/cp-0015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1141 - accuracy: 0.6106 - val_loss: 1.0659 - val_accuracy: 0.6355 - 1s/epoch - 13ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.06588\n",
      "88/88 - 1s - loss: 1.1088 - accuracy: 0.6139 - val_loss: 1.1509 - val_accuracy: 0.6081 - 601ms/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.06588\n",
      "88/88 - 1s - loss: 1.1022 - accuracy: 0.6147 - val_loss: 1.2480 - val_accuracy: 0.5690 - 587ms/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss improved from 1.06588 to 1.04667, saving model to ./cp/cp-0018.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0018.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0018.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0918 - accuracy: 0.6199 - val_loss: 1.0467 - val_accuracy: 0.6363 - 970ms/epoch - 11ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.04667\n",
      "88/88 - 1s - loss: 1.0875 - accuracy: 0.6210 - val_loss: 1.0739 - val_accuracy: 0.6262 - 574ms/epoch - 7ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.04667\n",
      "88/88 - 1s - loss: 1.0743 - accuracy: 0.6266 - val_loss: 1.0509 - val_accuracy: 0.6367 - 596ms/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.04667\n",
      "88/88 - 1s - loss: 1.0692 - accuracy: 0.6292 - val_loss: 1.1621 - val_accuracy: 0.5878 - 641ms/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss improved from 1.04667 to 1.02269, saving model to ./cp/cp-0022.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0669 - accuracy: 0.6289 - val_loss: 1.0227 - val_accuracy: 0.6517 - 988ms/epoch - 11ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss improved from 1.02269 to 0.98525, saving model to ./cp/cp-0023.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0584 - accuracy: 0.6330 - val_loss: 0.9853 - val_accuracy: 0.6646 - 1s/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.98525\n",
      "88/88 - 1s - loss: 1.0515 - accuracy: 0.6355 - val_loss: 1.0472 - val_accuracy: 0.6475 - 578ms/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.98525\n",
      "88/88 - 1s - loss: 1.0436 - accuracy: 0.6370 - val_loss: 1.0022 - val_accuracy: 0.6597 - 579ms/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.98525\n",
      "88/88 - 1s - loss: 1.0334 - accuracy: 0.6421 - val_loss: 1.1238 - val_accuracy: 0.6059 - 654ms/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.98525\n",
      "88/88 - 1s - loss: 1.0341 - accuracy: 0.6410 - val_loss: 1.0938 - val_accuracy: 0.6195 - 582ms/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.98525\n",
      "88/88 - 1s - loss: 1.0344 - accuracy: 0.6407 - val_loss: 1.0221 - val_accuracy: 0.6447 - 588ms/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.98525\n",
      "88/88 - 1s - loss: 1.0349 - accuracy: 0.6393 - val_loss: 1.0300 - val_accuracy: 0.6488 - 597ms/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss improved from 0.98525 to 0.96740, saving model to ./cp/cp-0030.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0258 - accuracy: 0.6422 - val_loss: 0.9674 - val_accuracy: 0.6701 - 960ms/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.96740\n",
      "88/88 - 1s - loss: 1.0206 - accuracy: 0.6464 - val_loss: 1.0186 - val_accuracy: 0.6538 - 567ms/epoch - 6ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.96740\n",
      "88/88 - 1s - loss: 1.0200 - accuracy: 0.6456 - val_loss: 1.0294 - val_accuracy: 0.6413 - 587ms/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.96740\n",
      "88/88 - 1s - loss: 1.0171 - accuracy: 0.6493 - val_loss: 0.9817 - val_accuracy: 0.6676 - 606ms/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss improved from 0.96740 to 0.94078, saving model to ./cp/cp-0034.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0034.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0034.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0110 - accuracy: 0.6461 - val_loss: 0.9408 - val_accuracy: 0.6783 - 957ms/epoch - 11ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 1.0078 - accuracy: 0.6507 - val_loss: 0.9838 - val_accuracy: 0.6612 - 580ms/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 1.0028 - accuracy: 0.6516 - val_loss: 0.9423 - val_accuracy: 0.6763 - 596ms/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 1.0130 - accuracy: 0.6458 - val_loss: 1.0158 - val_accuracy: 0.6447 - 579ms/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 1.0018 - accuracy: 0.6548 - val_loss: 1.0240 - val_accuracy: 0.6417 - 589ms/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 0.9988 - accuracy: 0.6548 - val_loss: 0.9682 - val_accuracy: 0.6730 - 566ms/epoch - 6ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 1.0002 - accuracy: 0.6512 - val_loss: 0.9583 - val_accuracy: 0.6696 - 583ms/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 0.9965 - accuracy: 0.6526 - val_loss: 0.9457 - val_accuracy: 0.6749 - 571ms/epoch - 6ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 0.9947 - accuracy: 0.6572 - val_loss: 0.9409 - val_accuracy: 0.6771 - 571ms/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 0.9878 - accuracy: 0.6581 - val_loss: 0.9546 - val_accuracy: 0.6717 - 624ms/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 0.9953 - accuracy: 0.6573 - val_loss: 1.0981 - val_accuracy: 0.6163 - 602ms/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 0.9904 - accuracy: 0.6581 - val_loss: 0.9585 - val_accuracy: 0.6697 - 582ms/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.94078\n",
      "88/88 - 1s - loss: 0.9810 - accuracy: 0.6575 - val_loss: 0.9659 - val_accuracy: 0.6686 - 581ms/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss improved from 0.94078 to 0.92645, saving model to ./cp/cp-0047.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0047.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0047.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 0.9829 - accuracy: 0.6608 - val_loss: 0.9265 - val_accuracy: 0.6839 - 1s/epoch - 13ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss improved from 0.92645 to 0.91733, saving model to ./cp/cp-0048.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0048.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0048.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 0.9748 - accuracy: 0.6596 - val_loss: 0.9173 - val_accuracy: 0.6862 - 963ms/epoch - 11ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.91733\n",
      "88/88 - 1s - loss: 0.9818 - accuracy: 0.6609 - val_loss: 0.9231 - val_accuracy: 0.6827 - 577ms/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.91733\n",
      "88/88 - 1s - loss: 0.9756 - accuracy: 0.6594 - val_loss: 0.9518 - val_accuracy: 0.6743 - 587ms/epoch - 7ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.2028048038482666, 1.6977267265319824, 1.5537662506103516, 1.4688512086868286, 1.4004020690917969, 1.3327701091766357, 1.2867817878723145, 1.248857855796814, 1.2217859029769897, 1.1871839761734009, 1.1712483167648315, 1.1533337831497192, 1.1398401260375977, 1.127013087272644, 1.1140645742416382, 1.10882568359375, 1.1021689176559448, 1.091784119606018, 1.087475061416626, 1.0743216276168823, 1.0692014694213867, 1.066894769668579, 1.0584392547607422, 1.0515387058258057, 1.0436487197875977, 1.0334206819534302, 1.0341356992721558, 1.0344293117523193, 1.0349233150482178, 1.025829792022705, 1.0206494331359863, 1.0200437307357788, 1.0171221494674683, 1.0109792947769165, 1.0078442096710205, 1.0028150081634521, 1.0130280256271362, 1.0018208026885986, 0.9987532496452332, 1.0001728534698486, 0.9965454339981079, 0.99466872215271, 0.9877904057502747, 0.9953148365020752, 0.99036705493927, 0.9809854030609131, 0.9828522205352783, 0.974837064743042, 0.9818089008331299, 0.9756444096565247]\n",
      "313/313 [==============================] - 0s 840us/step - loss: 0.8668 - accuracy: 0.7042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#3, opt: Adam, lr: 0.001, loss_fn: CCE, batch: 800\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 30, 30, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:48:03.014868: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:48:03.014889: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:48:03.109075: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:48:03.114097: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:48:03.511679: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_10/dropout_16/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:48:03.878337: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:48:03.878357: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:48:03.987136: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:48:03.991194: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:48:03.998208: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 122 callback api events and 123 activity events. \n",
      "2023-05-07 12:48:03.999554: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:48:03.999755: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_48_03/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.08905, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 2.3961 - accuracy: 0.2097 - val_loss: 2.0891 - val_accuracy: 0.2733 - 2s/epoch - 40ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 2.08905 to 1.94395, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.8497 - accuracy: 0.3287 - val_loss: 1.9439 - val_accuracy: 0.2952 - 898ms/epoch - 20ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 1.94395 to 1.79393, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.6699 - accuracy: 0.3917 - val_loss: 1.7939 - val_accuracy: 0.3674 - 1s/epoch - 24ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 1.79393 to 1.68544, saving model to ./cp/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.5530 - accuracy: 0.4380 - val_loss: 1.6854 - val_accuracy: 0.4061 - 916ms/epoch - 21ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.68544 to 1.52337, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.4816 - accuracy: 0.4652 - val_loss: 1.5234 - val_accuracy: 0.4633 - 913ms/epoch - 21ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.52337\n",
      "44/44 - 1s - loss: 1.4362 - accuracy: 0.4831 - val_loss: 1.5440 - val_accuracy: 0.4657 - 518ms/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss improved from 1.52337 to 1.48197, saving model to ./cp/cp-0007.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3989 - accuracy: 0.5022 - val_loss: 1.4820 - val_accuracy: 0.4830 - 901ms/epoch - 20ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.48197\n",
      "44/44 - 1s - loss: 1.3656 - accuracy: 0.5123 - val_loss: 1.6674 - val_accuracy: 0.4444 - 523ms/epoch - 12ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.48197\n",
      "44/44 - 1s - loss: 1.3332 - accuracy: 0.5271 - val_loss: 1.5416 - val_accuracy: 0.4719 - 548ms/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.48197\n",
      "44/44 - 1s - loss: 1.3076 - accuracy: 0.5349 - val_loss: 1.7131 - val_accuracy: 0.4484 - 548ms/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss improved from 1.48197 to 1.45064, saving model to ./cp/cp-0011.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2858 - accuracy: 0.5470 - val_loss: 1.4506 - val_accuracy: 0.5041 - 906ms/epoch - 21ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.45064\n",
      "44/44 - 1s - loss: 1.2677 - accuracy: 0.5531 - val_loss: 1.4988 - val_accuracy: 0.4912 - 571ms/epoch - 13ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss improved from 1.45064 to 1.31961, saving model to ./cp/cp-0013.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2569 - accuracy: 0.5567 - val_loss: 1.3196 - val_accuracy: 0.5459 - 898ms/epoch - 20ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss improved from 1.31961 to 1.24474, saving model to ./cp/cp-0014.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2343 - accuracy: 0.5656 - val_loss: 1.2447 - val_accuracy: 0.5708 - 1s/epoch - 25ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.24474\n",
      "44/44 - 1s - loss: 1.2198 - accuracy: 0.5706 - val_loss: 1.3476 - val_accuracy: 0.5365 - 545ms/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss improved from 1.24474 to 1.19563, saving model to ./cp/cp-0016.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2117 - accuracy: 0.5743 - val_loss: 1.1956 - val_accuracy: 0.5917 - 927ms/epoch - 21ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss improved from 1.19563 to 1.16462, saving model to ./cp/cp-0017.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0017.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0017.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2029 - accuracy: 0.5821 - val_loss: 1.1646 - val_accuracy: 0.6027 - 910ms/epoch - 21ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.16462\n",
      "44/44 - 1s - loss: 1.1897 - accuracy: 0.5824 - val_loss: 1.2281 - val_accuracy: 0.5793 - 541ms/epoch - 12ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss improved from 1.16462 to 1.14886, saving model to ./cp/cp-0019.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1822 - accuracy: 0.5854 - val_loss: 1.1489 - val_accuracy: 0.6095 - 892ms/epoch - 20ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.14886\n",
      "44/44 - 1s - loss: 1.1693 - accuracy: 0.5915 - val_loss: 1.2205 - val_accuracy: 0.5841 - 559ms/epoch - 13ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.14886\n",
      "44/44 - 1s - loss: 1.1659 - accuracy: 0.5927 - val_loss: 1.1519 - val_accuracy: 0.6079 - 545ms/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.14886\n",
      "44/44 - 1s - loss: 1.1540 - accuracy: 0.5953 - val_loss: 1.2085 - val_accuracy: 0.5841 - 529ms/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss improved from 1.14886 to 1.08607, saving model to ./cp/cp-0023.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1476 - accuracy: 0.5983 - val_loss: 1.0861 - val_accuracy: 0.6281 - 900ms/epoch - 20ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.08607\n",
      "44/44 - 1s - loss: 1.1385 - accuracy: 0.6029 - val_loss: 1.1033 - val_accuracy: 0.6223 - 533ms/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.08607\n",
      "44/44 - 1s - loss: 1.1380 - accuracy: 0.6029 - val_loss: 1.0884 - val_accuracy: 0.6277 - 531ms/epoch - 12ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.08607\n",
      "44/44 - 1s - loss: 1.1330 - accuracy: 0.6053 - val_loss: 1.1473 - val_accuracy: 0.6123 - 540ms/epoch - 12ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.08607\n",
      "44/44 - 1s - loss: 1.1285 - accuracy: 0.6069 - val_loss: 1.1145 - val_accuracy: 0.6103 - 528ms/epoch - 12ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss improved from 1.08607 to 1.08111, saving model to ./cp/cp-0028.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1218 - accuracy: 0.6091 - val_loss: 1.0811 - val_accuracy: 0.6282 - 936ms/epoch - 21ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.08111\n",
      "44/44 - 1s - loss: 1.1175 - accuracy: 0.6102 - val_loss: 1.1044 - val_accuracy: 0.6260 - 568ms/epoch - 13ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss improved from 1.08111 to 1.04100, saving model to ./cp/cp-0030.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1103 - accuracy: 0.6110 - val_loss: 1.0410 - val_accuracy: 0.6430 - 1s/epoch - 25ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss improved from 1.04100 to 1.03639, saving model to ./cp/cp-0031.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0031.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0031.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1073 - accuracy: 0.6138 - val_loss: 1.0364 - val_accuracy: 0.6437 - 948ms/epoch - 22ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.03639\n",
      "44/44 - 1s - loss: 1.1052 - accuracy: 0.6133 - val_loss: 1.0481 - val_accuracy: 0.6447 - 533ms/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.03639\n",
      "44/44 - 1s - loss: 1.1043 - accuracy: 0.6170 - val_loss: 1.0617 - val_accuracy: 0.6383 - 549ms/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.03639\n",
      "44/44 - 1s - loss: 1.0964 - accuracy: 0.6183 - val_loss: 1.0789 - val_accuracy: 0.6287 - 534ms/epoch - 12ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss improved from 1.03639 to 1.02550, saving model to ./cp/cp-0035.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0035.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0035.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0850 - accuracy: 0.6210 - val_loss: 1.0255 - val_accuracy: 0.6504 - 948ms/epoch - 22ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.02550\n",
      "44/44 - 1s - loss: 1.0851 - accuracy: 0.6214 - val_loss: 1.0294 - val_accuracy: 0.6488 - 577ms/epoch - 13ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.02550\n",
      "44/44 - 1s - loss: 1.0877 - accuracy: 0.6236 - val_loss: 1.0387 - val_accuracy: 0.6475 - 546ms/epoch - 12ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.02550\n",
      "44/44 - 1s - loss: 1.0817 - accuracy: 0.6269 - val_loss: 1.0300 - val_accuracy: 0.6458 - 551ms/epoch - 13ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss improved from 1.02550 to 1.01861, saving model to ./cp/cp-0039.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0039.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0039.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0734 - accuracy: 0.6254 - val_loss: 1.0186 - val_accuracy: 0.6531 - 936ms/epoch - 21ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.01861\n",
      "44/44 - 1s - loss: 1.0741 - accuracy: 0.6253 - val_loss: 1.1438 - val_accuracy: 0.6138 - 546ms/epoch - 12ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss improved from 1.01861 to 1.01765, saving model to ./cp/cp-0041.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0041.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0041.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0742 - accuracy: 0.6283 - val_loss: 1.0177 - val_accuracy: 0.6585 - 901ms/epoch - 20ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.01765\n",
      "44/44 - 1s - loss: 1.0675 - accuracy: 0.6316 - val_loss: 1.0597 - val_accuracy: 0.6357 - 532ms/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.01765\n",
      "44/44 - 1s - loss: 1.0700 - accuracy: 0.6293 - val_loss: 1.2050 - val_accuracy: 0.6005 - 535ms/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss improved from 1.01765 to 1.00733, saving model to ./cp/cp-0044.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0044.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0044.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0669 - accuracy: 0.6312 - val_loss: 1.0073 - val_accuracy: 0.6532 - 902ms/epoch - 21ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.00733\n",
      "44/44 - 1s - loss: 1.0607 - accuracy: 0.6332 - val_loss: 1.0113 - val_accuracy: 0.6593 - 536ms/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.00733\n",
      "44/44 - 1s - loss: 1.0564 - accuracy: 0.6363 - val_loss: 1.0619 - val_accuracy: 0.6385 - 535ms/epoch - 12ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.00733\n",
      "44/44 - 1s - loss: 1.0573 - accuracy: 0.6316 - val_loss: 1.0315 - val_accuracy: 0.6535 - 544ms/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.00733\n",
      "44/44 - 1s - loss: 1.0559 - accuracy: 0.6350 - val_loss: 1.0376 - val_accuracy: 0.6448 - 533ms/epoch - 12ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss improved from 1.00733 to 0.98212, saving model to ./cp/cp-0049.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0049.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0049.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0463 - accuracy: 0.6349 - val_loss: 0.9821 - val_accuracy: 0.6656 - 1s/epoch - 25ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.98212\n",
      "44/44 - 1s - loss: 1.0444 - accuracy: 0.6368 - val_loss: 0.9838 - val_accuracy: 0.6660 - 532ms/epoch - 12ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.396124839782715, 1.8496874570846558, 1.6698527336120605, 1.5529576539993286, 1.481550693511963, 1.4362086057662964, 1.3989074230194092, 1.3655503988265991, 1.3332182168960571, 1.307633399963379, 1.2857823371887207, 1.2677453756332397, 1.2569113969802856, 1.2342616319656372, 1.2198469638824463, 1.211669921875, 1.2028521299362183, 1.1896611452102661, 1.1822469234466553, 1.1693181991577148, 1.1658837795257568, 1.1539705991744995, 1.1476026773452759, 1.1385406255722046, 1.1380478143692017, 1.1329530477523804, 1.1284785270690918, 1.1217905282974243, 1.1175358295440674, 1.110320806503296, 1.1073428392410278, 1.105157494544983, 1.1043033599853516, 1.0964258909225464, 1.0849565267562866, 1.08512544631958, 1.0876609086990356, 1.0817317962646484, 1.0733720064163208, 1.0741294622421265, 1.0741668939590454, 1.0675208568572998, 1.070033311843872, 1.0669320821762085, 1.060660481452942, 1.0564384460449219, 1.0573174953460693, 1.055914282798767, 1.0463149547576904, 1.044404149055481]\n",
      "313/313 [==============================] - 0s 848us/step - loss: 0.9031 - accuracy: 0.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#4, opt: RMSprop, lr: 0.01, loss_fn: CCE, batch: 400\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_18 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 30, 30, 16)       64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:48:40.539081: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:48:40.539101: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:48:40.635264: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:48:40.642295: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:48:41.005839: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_11/dropout_18/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:48:41.414362: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:48:41.414381: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:48:41.521654: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:48:41.526361: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:48:41.533981: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 115 callback api events and 116 activity events. \n",
      "2023-05-07 12:48:41.535021: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:48:41.535126: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_48_41/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.32269, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 2s - loss: 2.5024 - accuracy: 0.2866 - val_loss: 2.3227 - val_accuracy: 0.3711 - 2s/epoch - 20ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss did not improve from 2.32269\n",
      "88/88 - 1s - loss: 1.6863 - accuracy: 0.4070 - val_loss: 2.4725 - val_accuracy: 0.3316 - 560ms/epoch - 6ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 2.32269 to 1.52025, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.5543 - accuracy: 0.4516 - val_loss: 1.5203 - val_accuracy: 0.4851 - 1s/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 1.52025 to 1.30143, saving model to ./cp/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.4846 - accuracy: 0.4784 - val_loss: 1.3014 - val_accuracy: 0.5384 - 1s/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.30143\n",
      "88/88 - 1s - loss: 1.4365 - accuracy: 0.4937 - val_loss: 1.4668 - val_accuracy: 0.4841 - 583ms/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.30143\n",
      "88/88 - 1s - loss: 1.3970 - accuracy: 0.5070 - val_loss: 1.6004 - val_accuracy: 0.4175 - 567ms/epoch - 6ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss improved from 1.30143 to 1.22892, saving model to ./cp/cp-0007.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.3877 - accuracy: 0.5150 - val_loss: 1.2289 - val_accuracy: 0.5685 - 918ms/epoch - 10ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.22892\n",
      "88/88 - 1s - loss: 1.3583 - accuracy: 0.5262 - val_loss: 1.2866 - val_accuracy: 0.5621 - 574ms/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss improved from 1.22892 to 1.20599, saving model to ./cp/cp-0009.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.3418 - accuracy: 0.5318 - val_loss: 1.2060 - val_accuracy: 0.5825 - 1s/epoch - 13ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss improved from 1.20599 to 1.18541, saving model to ./cp/cp-0010.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0010.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0010.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.3263 - accuracy: 0.5387 - val_loss: 1.1854 - val_accuracy: 0.5917 - 945ms/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.3152 - accuracy: 0.5429 - val_loss: 1.2824 - val_accuracy: 0.5367 - 606ms/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.3174 - accuracy: 0.5421 - val_loss: 1.6459 - val_accuracy: 0.4005 - 622ms/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.3054 - accuracy: 0.5485 - val_loss: 1.2490 - val_accuracy: 0.5818 - 641ms/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.2934 - accuracy: 0.5529 - val_loss: 1.2584 - val_accuracy: 0.5748 - 584ms/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.2902 - accuracy: 0.5536 - val_loss: 1.3555 - val_accuracy: 0.5462 - 612ms/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.2882 - accuracy: 0.5548 - val_loss: 1.2292 - val_accuracy: 0.5773 - 611ms/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.2831 - accuracy: 0.5557 - val_loss: 1.3065 - val_accuracy: 0.5630 - 594ms/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.18541\n",
      "88/88 - 1s - loss: 1.2849 - accuracy: 0.5576 - val_loss: 1.2381 - val_accuracy: 0.5781 - 573ms/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss improved from 1.18541 to 1.11510, saving model to ./cp/cp-0019.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2714 - accuracy: 0.5594 - val_loss: 1.1151 - val_accuracy: 0.6195 - 989ms/epoch - 11ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.11510\n",
      "88/88 - 1s - loss: 1.2696 - accuracy: 0.5603 - val_loss: 1.2690 - val_accuracy: 0.5545 - 607ms/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.11510\n",
      "88/88 - 1s - loss: 1.2815 - accuracy: 0.5597 - val_loss: 1.3510 - val_accuracy: 0.5450 - 651ms/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.11510\n",
      "88/88 - 1s - loss: 1.2641 - accuracy: 0.5630 - val_loss: 1.1510 - val_accuracy: 0.6161 - 602ms/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.11510\n",
      "88/88 - 1s - loss: 1.2573 - accuracy: 0.5711 - val_loss: 1.3414 - val_accuracy: 0.5336 - 618ms/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.11510\n",
      "88/88 - 1s - loss: 1.2550 - accuracy: 0.5650 - val_loss: 1.1280 - val_accuracy: 0.6074 - 594ms/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.11510\n",
      "88/88 - 1s - loss: 1.2563 - accuracy: 0.5707 - val_loss: 1.2761 - val_accuracy: 0.5869 - 598ms/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss improved from 1.11510 to 1.08262, saving model to ./cp/cp-0026.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0026.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0026.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2570 - accuracy: 0.5675 - val_loss: 1.0826 - val_accuracy: 0.6265 - 955ms/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.08262\n",
      "88/88 - 1s - loss: 1.2489 - accuracy: 0.5724 - val_loss: 1.2811 - val_accuracy: 0.5355 - 616ms/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.08262\n",
      "88/88 - 1s - loss: 1.2498 - accuracy: 0.5738 - val_loss: 1.1599 - val_accuracy: 0.6084 - 668ms/epoch - 8ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.08262\n",
      "88/88 - 1s - loss: 1.2473 - accuracy: 0.5727 - val_loss: 1.2270 - val_accuracy: 0.5916 - 599ms/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.08262\n",
      "88/88 - 1s - loss: 1.2475 - accuracy: 0.5716 - val_loss: 1.2176 - val_accuracy: 0.6030 - 591ms/epoch - 7ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.08262\n",
      "88/88 - 1s - loss: 1.2389 - accuracy: 0.5767 - val_loss: 1.0879 - val_accuracy: 0.6313 - 602ms/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss improved from 1.08262 to 1.07988, saving model to ./cp/cp-0032.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0032.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0032.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2399 - accuracy: 0.5757 - val_loss: 1.0799 - val_accuracy: 0.6332 - 923ms/epoch - 10ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2360 - accuracy: 0.5767 - val_loss: 1.1619 - val_accuracy: 0.6141 - 596ms/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2372 - accuracy: 0.5747 - val_loss: 1.3497 - val_accuracy: 0.5411 - 639ms/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2358 - accuracy: 0.5755 - val_loss: 1.2063 - val_accuracy: 0.5760 - 585ms/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2393 - accuracy: 0.5738 - val_loss: 1.0916 - val_accuracy: 0.6233 - 584ms/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2295 - accuracy: 0.5784 - val_loss: 1.2197 - val_accuracy: 0.5895 - 583ms/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2204 - accuracy: 0.5827 - val_loss: 1.2209 - val_accuracy: 0.5994 - 582ms/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2347 - accuracy: 0.5781 - val_loss: 1.2080 - val_accuracy: 0.5812 - 575ms/epoch - 7ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2221 - accuracy: 0.5820 - val_loss: 1.0924 - val_accuracy: 0.6331 - 611ms/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2142 - accuracy: 0.5844 - val_loss: 1.1214 - val_accuracy: 0.6121 - 565ms/epoch - 6ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2294 - accuracy: 0.5794 - val_loss: 1.1718 - val_accuracy: 0.5843 - 570ms/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2213 - accuracy: 0.5831 - val_loss: 1.1322 - val_accuracy: 0.6136 - 564ms/epoch - 6ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2171 - accuracy: 0.5825 - val_loss: 1.1706 - val_accuracy: 0.6249 - 561ms/epoch - 6ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.07988\n",
      "88/88 - 1s - loss: 1.2210 - accuracy: 0.5824 - val_loss: 1.1845 - val_accuracy: 0.5889 - 587ms/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss improved from 1.07988 to 1.06701, saving model to ./cp/cp-0046.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0046.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0046.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2309 - accuracy: 0.5777 - val_loss: 1.0670 - val_accuracy: 0.6319 - 931ms/epoch - 11ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.06701\n",
      "88/88 - 1s - loss: 1.2195 - accuracy: 0.5814 - val_loss: 1.0924 - val_accuracy: 0.6255 - 606ms/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.06701\n",
      "88/88 - 1s - loss: 1.2118 - accuracy: 0.5863 - val_loss: 1.1149 - val_accuracy: 0.6269 - 582ms/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.06701\n",
      "88/88 - 1s - loss: 1.2152 - accuracy: 0.5840 - val_loss: 1.2103 - val_accuracy: 0.6079 - 656ms/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.06701\n",
      "88/88 - 1s - loss: 1.2158 - accuracy: 0.5850 - val_loss: 1.1447 - val_accuracy: 0.6084 - 573ms/epoch - 7ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.502439498901367, 1.686293601989746, 1.5543196201324463, 1.4846454858779907, 1.4364659786224365, 1.3970199823379517, 1.3877367973327637, 1.358337640762329, 1.341774582862854, 1.326267957687378, 1.3151695728302002, 1.3173915147781372, 1.3053977489471436, 1.2933987379074097, 1.2902498245239258, 1.2881901264190674, 1.2831151485443115, 1.2849187850952148, 1.2713634967803955, 1.2696127891540527, 1.2814810276031494, 1.264136552810669, 1.2573246955871582, 1.255031704902649, 1.2562553882598877, 1.2569856643676758, 1.2488662004470825, 1.2497681379318237, 1.247271180152893, 1.2475125789642334, 1.238883376121521, 1.2398865222930908, 1.23603355884552, 1.2371882200241089, 1.2358102798461914, 1.2393115758895874, 1.2294857501983643, 1.2203617095947266, 1.234711766242981, 1.2221064567565918, 1.2142409086227417, 1.229378581047058, 1.221266508102417, 1.2170900106430054, 1.221007227897644, 1.2308666706085205, 1.2194652557373047, 1.2118473052978516, 1.215226650238037, 1.215790867805481]\n",
      "313/313 [==============================] - 0s 786us/step - loss: 1.0754 - accuracy: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#5, opt: RMSprop, lr: 0.01, loss_fn: CCE, batch: 800\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_20 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 30, 30, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:49:16.256764: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:49:16.256783: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:49:16.353708: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:49:16.361530: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:49:16.723032: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_12/dropout_20/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:49:17.002102: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:49:17.002122: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:49:17.112315: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:49:17.117606: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:49:17.125807: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 114 callback api events and 115 activity events. \n",
      "2023-05-07 12:49:17.126894: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:49:17.127019: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_49_17/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 4.33414, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 2.5515 - accuracy: 0.2529 - val_loss: 4.3341 - val_accuracy: 0.2207 - 2s/epoch - 36ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 4.33414 to 1.54563, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.7875 - accuracy: 0.3717 - val_loss: 1.5456 - val_accuracy: 0.4483 - 880ms/epoch - 20ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.54563\n",
      "44/44 - 1s - loss: 1.6214 - accuracy: 0.4287 - val_loss: 1.7535 - val_accuracy: 0.3728 - 526ms/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.54563\n",
      "44/44 - 1s - loss: 1.5811 - accuracy: 0.4486 - val_loss: 2.1968 - val_accuracy: 0.3159 - 517ms/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.54563 to 1.38961, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.4863 - accuracy: 0.4812 - val_loss: 1.3896 - val_accuracy: 0.5163 - 880ms/epoch - 20ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss improved from 1.38961 to 1.31928, saving model to ./cp/cp-0006.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.4568 - accuracy: 0.4895 - val_loss: 1.3193 - val_accuracy: 0.5421 - 868ms/epoch - 20ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.31928\n",
      "44/44 - 1s - loss: 1.4281 - accuracy: 0.5041 - val_loss: 1.7992 - val_accuracy: 0.3955 - 534ms/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.31928\n",
      "44/44 - 1s - loss: 1.4111 - accuracy: 0.5120 - val_loss: 1.4015 - val_accuracy: 0.5235 - 517ms/epoch - 12ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss improved from 1.31928 to 1.26552, saving model to ./cp/cp-0009.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3774 - accuracy: 0.5198 - val_loss: 1.2655 - val_accuracy: 0.5660 - 1s/epoch - 24ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.26552\n",
      "44/44 - 1s - loss: 1.3516 - accuracy: 0.5298 - val_loss: 1.5592 - val_accuracy: 0.4701 - 520ms/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.26552\n",
      "44/44 - 1s - loss: 1.3690 - accuracy: 0.5274 - val_loss: 1.3221 - val_accuracy: 0.5415 - 521ms/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss improved from 1.26552 to 1.18492, saving model to ./cp/cp-0012.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0012.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0012.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3346 - accuracy: 0.5362 - val_loss: 1.1849 - val_accuracy: 0.5839 - 945ms/epoch - 21ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss improved from 1.18492 to 1.15737, saving model to ./cp/cp-0013.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3226 - accuracy: 0.5365 - val_loss: 1.1574 - val_accuracy: 0.5973 - 888ms/epoch - 20ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.15737\n",
      "44/44 - 1s - loss: 1.2995 - accuracy: 0.5467 - val_loss: 1.2353 - val_accuracy: 0.5655 - 527ms/epoch - 12ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.15737\n",
      "44/44 - 1s - loss: 1.3097 - accuracy: 0.5445 - val_loss: 1.4958 - val_accuracy: 0.4699 - 525ms/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.15737\n",
      "44/44 - 1s - loss: 1.2991 - accuracy: 0.5493 - val_loss: 1.1710 - val_accuracy: 0.6020 - 515ms/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.15737\n",
      "44/44 - 1s - loss: 1.2868 - accuracy: 0.5563 - val_loss: 1.2497 - val_accuracy: 0.5641 - 536ms/epoch - 12ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.15737\n",
      "44/44 - 1s - loss: 1.2876 - accuracy: 0.5531 - val_loss: 1.1588 - val_accuracy: 0.5969 - 543ms/epoch - 12ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.15737\n",
      "44/44 - 1s - loss: 1.2729 - accuracy: 0.5611 - val_loss: 1.3406 - val_accuracy: 0.5465 - 538ms/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss improved from 1.15737 to 1.11880, saving model to ./cp/cp-0020.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0020.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0020.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2889 - accuracy: 0.5541 - val_loss: 1.1188 - val_accuracy: 0.6103 - 904ms/epoch - 21ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.11880\n",
      "44/44 - 1s - loss: 1.2790 - accuracy: 0.5605 - val_loss: 1.1855 - val_accuracy: 0.5922 - 530ms/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.11880\n",
      "44/44 - 1s - loss: 1.2692 - accuracy: 0.5595 - val_loss: 1.2927 - val_accuracy: 0.5565 - 530ms/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.11880\n",
      "44/44 - 1s - loss: 1.2767 - accuracy: 0.5592 - val_loss: 1.2054 - val_accuracy: 0.5860 - 520ms/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.11880\n",
      "44/44 - 1s - loss: 1.2645 - accuracy: 0.5616 - val_loss: 1.1365 - val_accuracy: 0.6100 - 529ms/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.11880\n",
      "44/44 - 1s - loss: 1.2670 - accuracy: 0.5619 - val_loss: 1.1808 - val_accuracy: 0.5889 - 533ms/epoch - 12ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.11880\n",
      "44/44 - 1s - loss: 1.2651 - accuracy: 0.5634 - val_loss: 1.2934 - val_accuracy: 0.5751 - 542ms/epoch - 12ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss improved from 1.11880 to 1.11747, saving model to ./cp/cp-0027.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0027.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0027.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2636 - accuracy: 0.5616 - val_loss: 1.1175 - val_accuracy: 0.6166 - 898ms/epoch - 20ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss improved from 1.11747 to 1.11683, saving model to ./cp/cp-0028.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2483 - accuracy: 0.5728 - val_loss: 1.1168 - val_accuracy: 0.6194 - 850ms/epoch - 19ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.11683\n",
      "44/44 - 1s - loss: 1.2452 - accuracy: 0.5679 - val_loss: 1.4207 - val_accuracy: 0.5192 - 520ms/epoch - 12ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.11683\n",
      "44/44 - 1s - loss: 1.2454 - accuracy: 0.5727 - val_loss: 1.1320 - val_accuracy: 0.6120 - 514ms/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss improved from 1.11683 to 1.11119, saving model to ./cp/cp-0031.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0031.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0031.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2491 - accuracy: 0.5671 - val_loss: 1.1112 - val_accuracy: 0.6152 - 886ms/epoch - 20ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.11119\n",
      "44/44 - 1s - loss: 1.2466 - accuracy: 0.5714 - val_loss: 1.3696 - val_accuracy: 0.5445 - 543ms/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.11119\n",
      "44/44 - 1s - loss: 1.2477 - accuracy: 0.5703 - val_loss: 1.2429 - val_accuracy: 0.5642 - 531ms/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.11119\n",
      "44/44 - 1s - loss: 1.2539 - accuracy: 0.5661 - val_loss: 1.2765 - val_accuracy: 0.5678 - 542ms/epoch - 12ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss improved from 1.11119 to 1.11084, saving model to ./cp/cp-0035.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0035.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0035.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2420 - accuracy: 0.5712 - val_loss: 1.1108 - val_accuracy: 0.6251 - 1s/epoch - 25ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.11084\n",
      "44/44 - 1s - loss: 1.2462 - accuracy: 0.5722 - val_loss: 1.2903 - val_accuracy: 0.5685 - 530ms/epoch - 12ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.11084\n",
      "44/44 - 1s - loss: 1.2420 - accuracy: 0.5703 - val_loss: 1.1633 - val_accuracy: 0.6127 - 523ms/epoch - 12ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.11084\n",
      "44/44 - 1s - loss: 1.2386 - accuracy: 0.5730 - val_loss: 1.3519 - val_accuracy: 0.5380 - 557ms/epoch - 13ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.11084\n",
      "44/44 - 1s - loss: 1.2393 - accuracy: 0.5754 - val_loss: 1.1735 - val_accuracy: 0.6034 - 552ms/epoch - 13ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.11084\n",
      "44/44 - 1s - loss: 1.2348 - accuracy: 0.5738 - val_loss: 1.2863 - val_accuracy: 0.5629 - 543ms/epoch - 12ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.11084\n",
      "44/44 - 1s - loss: 1.2347 - accuracy: 0.5783 - val_loss: 1.3665 - val_accuracy: 0.5590 - 518ms/epoch - 12ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.11084\n",
      "44/44 - 1s - loss: 1.2414 - accuracy: 0.5713 - val_loss: 1.1635 - val_accuracy: 0.6043 - 523ms/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss improved from 1.11084 to 1.08031, saving model to ./cp/cp-0043.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0043.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0043.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2318 - accuracy: 0.5757 - val_loss: 1.0803 - val_accuracy: 0.6317 - 921ms/epoch - 21ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.08031\n",
      "44/44 - 1s - loss: 1.2297 - accuracy: 0.5789 - val_loss: 1.2013 - val_accuracy: 0.5889 - 541ms/epoch - 12ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.08031\n",
      "44/44 - 1s - loss: 1.2282 - accuracy: 0.5763 - val_loss: 1.1327 - val_accuracy: 0.6082 - 523ms/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.08031\n",
      "44/44 - 1s - loss: 1.2255 - accuracy: 0.5779 - val_loss: 1.1918 - val_accuracy: 0.6000 - 503ms/epoch - 11ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.08031\n",
      "44/44 - 1s - loss: 1.2363 - accuracy: 0.5778 - val_loss: 1.0830 - val_accuracy: 0.6290 - 519ms/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.08031\n",
      "44/44 - 1s - loss: 1.2246 - accuracy: 0.5773 - val_loss: 1.1663 - val_accuracy: 0.6151 - 521ms/epoch - 12ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.08031\n",
      "44/44 - 1s - loss: 1.2295 - accuracy: 0.5770 - val_loss: 1.2468 - val_accuracy: 0.5659 - 528ms/epoch - 12ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.08031\n",
      "44/44 - 1s - loss: 1.2211 - accuracy: 0.5811 - val_loss: 1.2163 - val_accuracy: 0.6012 - 540ms/epoch - 12ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.551490306854248, 1.7875293493270874, 1.6213665008544922, 1.5810867547988892, 1.4862866401672363, 1.4567986726760864, 1.4280766248703003, 1.4110571146011353, 1.3774199485778809, 1.3515909910202026, 1.3690218925476074, 1.3345701694488525, 1.3226330280303955, 1.2994853258132935, 1.3097000122070312, 1.2991218566894531, 1.2867885828018188, 1.2876017093658447, 1.2729159593582153, 1.2888665199279785, 1.2789772748947144, 1.2692123651504517, 1.2767075300216675, 1.2645477056503296, 1.2670016288757324, 1.2651265859603882, 1.2636446952819824, 1.2483248710632324, 1.2451614141464233, 1.2453733682632446, 1.2490910291671753, 1.2465893030166626, 1.247687816619873, 1.2538878917694092, 1.2419936656951904, 1.246187448501587, 1.2419626712799072, 1.238556981086731, 1.23933744430542, 1.2347856760025024, 1.2346781492233276, 1.2413710355758667, 1.2318114042282104, 1.2297383546829224, 1.2282154560089111, 1.225470781326294, 1.2363258600234985, 1.2245866060256958, 1.2295421361923218, 1.2211076021194458]\n",
      "313/313 [==============================] - 0s 760us/step - loss: 1.1101 - accuracy: 0.6286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#6, opt: RMSprop, lr: 0.001, loss_fn: CCE, batch: 400\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_22 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 30, 30, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_11 (Flatten)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:49:49.502607: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:49:49.502627: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:49:49.593856: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:49:49.599921: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:49:49.961691: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_13/dropout_22/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:49:50.237182: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:49:50.237200: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:49:50.342032: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:49:50.346881: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:49:50.354227: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 115 callback api events and 116 activity events. \n",
      "2023-05-07 12:49:50.355237: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:49:50.355355: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_49_50/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.97101, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 2s - loss: 2.1087 - accuracy: 0.2811 - val_loss: 1.9710 - val_accuracy: 0.3249 - 2s/epoch - 19ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 1.97101 to 1.62588, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.5898 - accuracy: 0.4235 - val_loss: 1.6259 - val_accuracy: 0.4217 - 909ms/epoch - 10ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 1.62588 to 1.41098, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.4568 - accuracy: 0.4755 - val_loss: 1.4110 - val_accuracy: 0.5021 - 1s/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.41098\n",
      "88/88 - 1s - loss: 1.3877 - accuracy: 0.5053 - val_loss: 1.4314 - val_accuracy: 0.5027 - 590ms/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.41098 to 1.34885, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.3342 - accuracy: 0.5275 - val_loss: 1.3489 - val_accuracy: 0.5313 - 982ms/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.34885\n",
      "88/88 - 1s - loss: 1.2906 - accuracy: 0.5458 - val_loss: 1.8451 - val_accuracy: 0.4113 - 581ms/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss improved from 1.34885 to 1.19575, saving model to ./cp/cp-0007.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.2641 - accuracy: 0.5550 - val_loss: 1.1957 - val_accuracy: 0.5914 - 992ms/epoch - 11ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.19575\n",
      "88/88 - 1s - loss: 1.2400 - accuracy: 0.5639 - val_loss: 1.2698 - val_accuracy: 0.5611 - 568ms/epoch - 6ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.19575\n",
      "88/88 - 1s - loss: 1.2122 - accuracy: 0.5741 - val_loss: 1.1990 - val_accuracy: 0.5846 - 566ms/epoch - 6ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.19575\n",
      "88/88 - 1s - loss: 1.2008 - accuracy: 0.5809 - val_loss: 1.2270 - val_accuracy: 0.5763 - 587ms/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss improved from 1.19575 to 1.10718, saving model to ./cp/cp-0011.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1823 - accuracy: 0.5893 - val_loss: 1.1072 - val_accuracy: 0.6203 - 965ms/epoch - 11ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.10718\n",
      "88/88 - 1s - loss: 1.1715 - accuracy: 0.5909 - val_loss: 1.1907 - val_accuracy: 0.5876 - 680ms/epoch - 8ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.10718\n",
      "88/88 - 1s - loss: 1.1599 - accuracy: 0.5987 - val_loss: 1.2083 - val_accuracy: 0.5925 - 600ms/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.10718\n",
      "88/88 - 1s - loss: 1.1442 - accuracy: 0.5993 - val_loss: 1.2009 - val_accuracy: 0.5870 - 587ms/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.10718\n",
      "88/88 - 1s - loss: 1.1405 - accuracy: 0.6045 - val_loss: 1.1759 - val_accuracy: 0.5999 - 553ms/epoch - 6ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss improved from 1.10718 to 1.10154, saving model to ./cp/cp-0016.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.1254 - accuracy: 0.6048 - val_loss: 1.1015 - val_accuracy: 0.6107 - 924ms/epoch - 10ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.10154\n",
      "88/88 - 1s - loss: 1.1177 - accuracy: 0.6103 - val_loss: 1.1676 - val_accuracy: 0.5946 - 545ms/epoch - 6ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.10154\n",
      "88/88 - 1s - loss: 1.1133 - accuracy: 0.6131 - val_loss: 1.1063 - val_accuracy: 0.6216 - 564ms/epoch - 6ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss did not improve from 1.10154\n",
      "88/88 - 1s - loss: 1.1009 - accuracy: 0.6159 - val_loss: 1.1490 - val_accuracy: 0.5947 - 568ms/epoch - 6ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss improved from 1.10154 to 1.07436, saving model to ./cp/cp-0020.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0020.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0020.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0980 - accuracy: 0.6215 - val_loss: 1.0744 - val_accuracy: 0.6401 - 909ms/epoch - 10ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.07436\n",
      "88/88 - 1s - loss: 1.0967 - accuracy: 0.6192 - val_loss: 1.1246 - val_accuracy: 0.6143 - 602ms/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss improved from 1.07436 to 1.06039, saving model to ./cp/cp-0022.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0903 - accuracy: 0.6240 - val_loss: 1.0604 - val_accuracy: 0.6452 - 909ms/epoch - 10ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss improved from 1.06039 to 1.01200, saving model to ./cp/cp-0023.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0023.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0837 - accuracy: 0.6247 - val_loss: 1.0120 - val_accuracy: 0.6563 - 1s/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.01200\n",
      "88/88 - 1s - loss: 1.0764 - accuracy: 0.6265 - val_loss: 1.0148 - val_accuracy: 0.6532 - 552ms/epoch - 6ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.01200\n",
      "88/88 - 1s - loss: 1.0738 - accuracy: 0.6285 - val_loss: 1.0190 - val_accuracy: 0.6484 - 618ms/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.01200\n",
      "88/88 - 1s - loss: 1.0717 - accuracy: 0.6283 - val_loss: 1.0511 - val_accuracy: 0.6399 - 620ms/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.01200\n",
      "88/88 - 1s - loss: 1.0642 - accuracy: 0.6327 - val_loss: 1.1198 - val_accuracy: 0.6179 - 622ms/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.01200\n",
      "88/88 - 1s - loss: 1.0632 - accuracy: 0.6311 - val_loss: 1.0311 - val_accuracy: 0.6411 - 588ms/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.01200\n",
      "88/88 - 1s - loss: 1.0606 - accuracy: 0.6338 - val_loss: 1.0353 - val_accuracy: 0.6510 - 600ms/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss improved from 1.01200 to 1.00154, saving model to ./cp/cp-0030.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0030.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0506 - accuracy: 0.6381 - val_loss: 1.0015 - val_accuracy: 0.6581 - 996ms/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0481 - accuracy: 0.6369 - val_loss: 1.0199 - val_accuracy: 0.6459 - 710ms/epoch - 8ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0507 - accuracy: 0.6372 - val_loss: 1.0500 - val_accuracy: 0.6368 - 567ms/epoch - 6ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0460 - accuracy: 0.6367 - val_loss: 1.2146 - val_accuracy: 0.5719 - 566ms/epoch - 6ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0455 - accuracy: 0.6405 - val_loss: 1.0721 - val_accuracy: 0.6335 - 622ms/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0365 - accuracy: 0.6439 - val_loss: 1.1232 - val_accuracy: 0.6246 - 582ms/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0385 - accuracy: 0.6423 - val_loss: 1.0059 - val_accuracy: 0.6519 - 585ms/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0354 - accuracy: 0.6386 - val_loss: 1.0837 - val_accuracy: 0.6300 - 612ms/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0289 - accuracy: 0.6442 - val_loss: 1.5632 - val_accuracy: 0.5045 - 623ms/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 1.00154\n",
      "88/88 - 1s - loss: 1.0278 - accuracy: 0.6452 - val_loss: 1.0552 - val_accuracy: 0.6352 - 565ms/epoch - 6ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss improved from 1.00154 to 0.95551, saving model to ./cp/cp-0040.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0040.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0040.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0242 - accuracy: 0.6468 - val_loss: 0.9555 - val_accuracy: 0.6707 - 924ms/epoch - 11ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.95551\n",
      "88/88 - 1s - loss: 1.0264 - accuracy: 0.6459 - val_loss: 1.1399 - val_accuracy: 0.6061 - 601ms/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.95551\n",
      "88/88 - 1s - loss: 1.0192 - accuracy: 0.6509 - val_loss: 1.0372 - val_accuracy: 0.6437 - 589ms/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.95551\n",
      "88/88 - 1s - loss: 1.0174 - accuracy: 0.6457 - val_loss: 0.9970 - val_accuracy: 0.6636 - 580ms/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.95551\n",
      "88/88 - 1s - loss: 1.0161 - accuracy: 0.6489 - val_loss: 1.4265 - val_accuracy: 0.5379 - 629ms/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.95551\n",
      "88/88 - 1s - loss: 1.0159 - accuracy: 0.6507 - val_loss: 0.9821 - val_accuracy: 0.6672 - 591ms/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.95551\n",
      "88/88 - 1s - loss: 1.0153 - accuracy: 0.6479 - val_loss: 1.1289 - val_accuracy: 0.6213 - 632ms/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss improved from 0.95551 to 0.94746, saving model to ./cp/cp-0047.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0047.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0047.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 - 1s - loss: 1.0110 - accuracy: 0.6503 - val_loss: 0.9475 - val_accuracy: 0.6779 - 946ms/epoch - 11ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.94746\n",
      "88/88 - 1s - loss: 1.0023 - accuracy: 0.6526 - val_loss: 1.0371 - val_accuracy: 0.6395 - 594ms/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.94746\n",
      "88/88 - 1s - loss: 1.0062 - accuracy: 0.6530 - val_loss: 1.1708 - val_accuracy: 0.5963 - 614ms/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.94746\n",
      "88/88 - 1s - loss: 1.0101 - accuracy: 0.6504 - val_loss: 1.2599 - val_accuracy: 0.5778 - 587ms/epoch - 7ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.1086628437042236, 1.5898422002792358, 1.4567978382110596, 1.3876769542694092, 1.3342007398605347, 1.2905737161636353, 1.2640551328659058, 1.2399814128875732, 1.2121537923812866, 1.200751543045044, 1.1823272705078125, 1.1714504957199097, 1.1599265336990356, 1.1442232131958008, 1.140485167503357, 1.1253951787948608, 1.1176669597625732, 1.1133290529251099, 1.1008692979812622, 1.0980393886566162, 1.0966601371765137, 1.090326189994812, 1.0837082862854004, 1.0764344930648804, 1.0738213062286377, 1.0717064142227173, 1.064194917678833, 1.0632219314575195, 1.0605733394622803, 1.0506243705749512, 1.0481250286102295, 1.0507317781448364, 1.0460087060928345, 1.045486330986023, 1.0364657640457153, 1.0384595394134521, 1.0353959798812866, 1.028891682624817, 1.027787446975708, 1.024236798286438, 1.026403546333313, 1.0192433595657349, 1.0173516273498535, 1.0160833597183228, 1.015851378440857, 1.0153381824493408, 1.0109716653823853, 1.002292513847351, 1.0062159299850464, 1.0101020336151123]\n",
      "313/313 [==============================] - 0s 848us/step - loss: 1.1716 - accuracy: 0.6081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "#7, opt: RMSprop, lr: 0.001, loss_fn: CCE, batch: 800\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 30, 30, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:50:25.848908: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:50:25.848928: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:50:25.942057: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:50:25.948130: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:50:26.344760: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_14/dropout_24/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:50:26.625805: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:50:26.625822: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:50:26.732062: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:50:26.736980: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:50:26.744110: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 114 callback api events and 115 activity events. \n",
      "2023-05-07 12:50:26.745082: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:50:26.745196: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_50_26/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 1.95878, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 2.4163 - accuracy: 0.2080 - val_loss: 1.9588 - val_accuracy: 0.2880 - 2s/epoch - 37ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss did not improve from 1.95878\n",
      "44/44 - 1s - loss: 1.8133 - accuracy: 0.3366 - val_loss: 1.9881 - val_accuracy: 0.3317 - 525ms/epoch - 12ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss did not improve from 1.95878\n",
      "44/44 - 1s - loss: 1.6345 - accuracy: 0.4071 - val_loss: 1.9693 - val_accuracy: 0.3548 - 531ms/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.95878\n",
      "44/44 - 1s - loss: 1.5253 - accuracy: 0.4487 - val_loss: 2.2106 - val_accuracy: 0.3529 - 529ms/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.95878 to 1.62123, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.4597 - accuracy: 0.4740 - val_loss: 1.6212 - val_accuracy: 0.4452 - 1s/epoch - 25ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss did not improve from 1.62123\n",
      "44/44 - 1s - loss: 1.4001 - accuracy: 0.5011 - val_loss: 1.6339 - val_accuracy: 0.4507 - 532ms/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.62123\n",
      "44/44 - 1s - loss: 1.3642 - accuracy: 0.5145 - val_loss: 1.7245 - val_accuracy: 0.4342 - 529ms/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss improved from 1.62123 to 1.45386, saving model to ./cp/cp-0008.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3257 - accuracy: 0.5301 - val_loss: 1.4539 - val_accuracy: 0.4955 - 900ms/epoch - 20ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss improved from 1.45386 to 1.25953, saving model to ./cp/cp-0009.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.3025 - accuracy: 0.5417 - val_loss: 1.2595 - val_accuracy: 0.5604 - 904ms/epoch - 21ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.25953\n",
      "44/44 - 1s - loss: 1.2730 - accuracy: 0.5532 - val_loss: 1.3053 - val_accuracy: 0.5445 - 536ms/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.25953\n",
      "44/44 - 1s - loss: 1.2534 - accuracy: 0.5622 - val_loss: 1.5008 - val_accuracy: 0.4935 - 544ms/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.25953\n",
      "44/44 - 1s - loss: 1.2438 - accuracy: 0.5636 - val_loss: 1.3336 - val_accuracy: 0.5439 - 536ms/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss improved from 1.25953 to 1.20910, saving model to ./cp/cp-0013.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.2224 - accuracy: 0.5717 - val_loss: 1.2091 - val_accuracy: 0.5869 - 896ms/epoch - 20ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.20910\n",
      "44/44 - 1s - loss: 1.2109 - accuracy: 0.5783 - val_loss: 1.4261 - val_accuracy: 0.4992 - 540ms/epoch - 12ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.20910\n",
      "44/44 - 1s - loss: 1.1948 - accuracy: 0.5837 - val_loss: 1.2494 - val_accuracy: 0.5717 - 525ms/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss improved from 1.20910 to 1.19879, saving model to ./cp/cp-0016.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0016.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1915 - accuracy: 0.5846 - val_loss: 1.1988 - val_accuracy: 0.5779 - 893ms/epoch - 20ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.19879\n",
      "44/44 - 1s - loss: 1.1810 - accuracy: 0.5876 - val_loss: 1.2504 - val_accuracy: 0.5623 - 547ms/epoch - 12ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss improved from 1.19879 to 1.18981, saving model to ./cp/cp-0018.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0018.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0018.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1751 - accuracy: 0.5906 - val_loss: 1.1898 - val_accuracy: 0.5947 - 896ms/epoch - 20ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss improved from 1.18981 to 1.10813, saving model to ./cp/cp-0019.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1669 - accuracy: 0.5947 - val_loss: 1.1081 - val_accuracy: 0.6223 - 1s/epoch - 25ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.10813\n",
      "44/44 - 1s - loss: 1.1497 - accuracy: 0.6013 - val_loss: 1.1551 - val_accuracy: 0.5957 - 534ms/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 1.10813\n",
      "44/44 - 1s - loss: 1.1505 - accuracy: 0.6011 - val_loss: 1.1906 - val_accuracy: 0.5981 - 525ms/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss improved from 1.10813 to 1.10604, saving model to ./cp/cp-0022.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0022.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1437 - accuracy: 0.6021 - val_loss: 1.1060 - val_accuracy: 0.6150 - 933ms/epoch - 21ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.10604\n",
      "44/44 - 1s - loss: 1.1323 - accuracy: 0.6078 - val_loss: 1.2894 - val_accuracy: 0.5410 - 531ms/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.10604\n",
      "44/44 - 1s - loss: 1.1340 - accuracy: 0.6035 - val_loss: 1.4689 - val_accuracy: 0.4911 - 524ms/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.10604\n",
      "44/44 - 1s - loss: 1.1219 - accuracy: 0.6089 - val_loss: 1.1157 - val_accuracy: 0.6115 - 553ms/epoch - 13ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss improved from 1.10604 to 1.09312, saving model to ./cp/cp-0026.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0026.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0026.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1192 - accuracy: 0.6115 - val_loss: 1.0931 - val_accuracy: 0.6199 - 893ms/epoch - 20ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.09312\n",
      "44/44 - 1s - loss: 1.1158 - accuracy: 0.6152 - val_loss: 1.1301 - val_accuracy: 0.6187 - 538ms/epoch - 12ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss did not improve from 1.09312\n",
      "44/44 - 1s - loss: 1.1031 - accuracy: 0.6149 - val_loss: 1.1234 - val_accuracy: 0.6085 - 532ms/epoch - 12ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 1.09312\n",
      "44/44 - 1s - loss: 1.1076 - accuracy: 0.6146 - val_loss: 1.5440 - val_accuracy: 0.5125 - 522ms/epoch - 12ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss did not improve from 1.09312\n",
      "44/44 - 1s - loss: 1.1025 - accuracy: 0.6148 - val_loss: 1.1329 - val_accuracy: 0.6104 - 535ms/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss improved from 1.09312 to 1.04661, saving model to ./cp/cp-0031.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0031.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0031.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.1009 - accuracy: 0.6178 - val_loss: 1.0466 - val_accuracy: 0.6391 - 892ms/epoch - 20ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 1.04661\n",
      "44/44 - 1s - loss: 1.0883 - accuracy: 0.6208 - val_loss: 1.1496 - val_accuracy: 0.5999 - 533ms/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 1.04661\n",
      "44/44 - 1s - loss: 1.0920 - accuracy: 0.6204 - val_loss: 1.1240 - val_accuracy: 0.6125 - 529ms/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss improved from 1.04661 to 1.03722, saving model to ./cp/cp-0034.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0034.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0034.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0894 - accuracy: 0.6206 - val_loss: 1.0372 - val_accuracy: 0.6439 - 902ms/epoch - 21ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 1.03722\n",
      "44/44 - 1s - loss: 1.0800 - accuracy: 0.6261 - val_loss: 1.0893 - val_accuracy: 0.6253 - 539ms/epoch - 12ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 1.03722\n",
      "44/44 - 1s - loss: 1.0781 - accuracy: 0.6229 - val_loss: 1.0669 - val_accuracy: 0.6379 - 555ms/epoch - 13ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 1.03722\n",
      "44/44 - 1s - loss: 1.0729 - accuracy: 0.6282 - val_loss: 1.0519 - val_accuracy: 0.6372 - 540ms/epoch - 12ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 1.03722\n",
      "44/44 - 1s - loss: 1.0728 - accuracy: 0.6299 - val_loss: 1.2221 - val_accuracy: 0.5749 - 543ms/epoch - 12ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss improved from 1.03722 to 1.02131, saving model to ./cp/cp-0039.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0039.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0039.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 1s - loss: 1.0730 - accuracy: 0.6286 - val_loss: 1.0213 - val_accuracy: 0.6472 - 889ms/epoch - 20ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0615 - accuracy: 0.6350 - val_loss: 1.1865 - val_accuracy: 0.5799 - 532ms/epoch - 12ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0659 - accuracy: 0.6302 - val_loss: 1.0999 - val_accuracy: 0.6159 - 521ms/epoch - 12ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0647 - accuracy: 0.6304 - val_loss: 1.1308 - val_accuracy: 0.6028 - 536ms/epoch - 12ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0590 - accuracy: 0.6323 - val_loss: 1.0459 - val_accuracy: 0.6391 - 540ms/epoch - 12ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0522 - accuracy: 0.6357 - val_loss: 1.2739 - val_accuracy: 0.5614 - 535ms/epoch - 12ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0495 - accuracy: 0.6360 - val_loss: 1.1185 - val_accuracy: 0.6252 - 528ms/epoch - 12ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0524 - accuracy: 0.6355 - val_loss: 1.1999 - val_accuracy: 0.5831 - 538ms/epoch - 12ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0542 - accuracy: 0.6353 - val_loss: 1.1173 - val_accuracy: 0.6115 - 524ms/epoch - 12ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0442 - accuracy: 0.6388 - val_loss: 1.1422 - val_accuracy: 0.6045 - 551ms/epoch - 13ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0436 - accuracy: 0.6378 - val_loss: 1.1144 - val_accuracy: 0.6106 - 539ms/epoch - 12ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 1.02131\n",
      "44/44 - 1s - loss: 1.0413 - accuracy: 0.6390 - val_loss: 1.1101 - val_accuracy: 0.6103 - 530ms/epoch - 12ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.4163031578063965, 1.8133281469345093, 1.6345094442367554, 1.525344967842102, 1.4597474336624146, 1.4000893831253052, 1.3642423152923584, 1.3257195949554443, 1.3024928569793701, 1.2730063199996948, 1.2533880472183228, 1.24383544921875, 1.2223812341690063, 1.2108701467514038, 1.19479501247406, 1.191532850265503, 1.1810394525527954, 1.1751282215118408, 1.1668617725372314, 1.1496751308441162, 1.150460124015808, 1.143737554550171, 1.13230299949646, 1.1340373754501343, 1.1218903064727783, 1.1192376613616943, 1.1158180236816406, 1.103144884109497, 1.107649326324463, 1.102486491203308, 1.1008938550949097, 1.088256597518921, 1.0920124053955078, 1.0893962383270264, 1.0800092220306396, 1.078148603439331, 1.0729179382324219, 1.0727825164794922, 1.073047399520874, 1.06150484085083, 1.0658907890319824, 1.0646730661392212, 1.0590113401412964, 1.0522401332855225, 1.0495294332504272, 1.0523688793182373, 1.0541902780532837, 1.0441601276397705, 1.0436313152313232, 1.0413179397583008]\n",
      "313/313 [==============================] - 0s 819us/step - loss: 1.0245 - accuracy: 0.6413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_24 (Conv2D)          (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 30, 30, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 15, 15, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_25 (MaxPoolin  (None, 6, 6, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                11530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,682\n",
      "Trainable params: 16,650\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax', 'callbacks': [<keras.callbacks.ModelCheckpoint object at 0x7fc43ac91390>, <keras.callbacks.TensorBoard object at 0x7fc43d632e60>]}\n",
      "MinMax\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "result = benchmark.benchmark(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zcqN8BT3uBx"
   },
   "source": [
    "## 146K CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lgzRym6MXuE"
   },
   "outputs": [],
   "source": [
    "# {'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax', 'callbacks': [<keras.callbacks.ModelCheckpoint object at 0x7fc43ac91390>, <keras.callbacks.TensorBoard object at 0x7fc43d632e60>]}\n",
    "# MinMax\n",
    "\n",
    "class Cifar10Benchmark2(Cifar10Benchmark):\n",
    "    def __init__(self, name='Cifar', train_size=600):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def make_hyper_params(self):\n",
    "        self.params = {\n",
    "            'opt': ['RMSprop'],\n",
    "            'lr': [0.001],\n",
    "            'loss_fn': ['CCE'],\n",
    "            'metrics': ['accuracy'],\n",
    "            'batch': [800],\n",
    "            'validation_split': [0.3],\n",
    "            'verbose': [2],\n",
    "            'scaler': ['MinMax'],\n",
    "        }\n",
    "        import itertools\n",
    "        permutations_dicts = [dict(zip(self.params.keys(), v))  for v in itertools.product(*self.params.values())]\n",
    "        print(permutations_dicts)\n",
    "        return permutations_dicts\n",
    "\n",
    "    def make_model(self, opt='Adam', lr='0.01', loss_fn='MSE', **kargs):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(Input(self.input_shape)) # shape=(32, 32, 3)\n",
    "\n",
    "        self.model.add(Conv2D(filters=16, kernel_size = (7,7), padding='same', activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPool2D())\n",
    "        self.model.add(Dropout(rate=0.2))\n",
    "\n",
    "        self.model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu')) \n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPool2D())\n",
    "        self.model.add(Dropout(rate=0.2))\n",
    "\n",
    "        self.model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')) \n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPool2D())\n",
    "        self.model.add(Dropout(rate=0.2))\n",
    "\n",
    "        self.model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')) \n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPool2D())\n",
    "        self.model.add(Dropout(rate=0.2))\n",
    "\n",
    "        self.model.add(Conv2D(filters=256, kernel_size=(1,1), padding='same', activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(tf.keras.layers.GlobalMaxPool2D())\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(rate=0.5))\n",
    "        self.model.add(Dense(units=10, activation='softmax')) # 10 classes \n",
    "        self.model.summary()\n",
    "        \n",
    "        optimizer = self.make_optimizer(opt, lr)\n",
    "        loss_func = self.make_loss_func(loss_fn)\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_func, metrics=kargs['metrics'])\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NcAfnHMVd8Z",
    "outputId": "84b2db93-5e2f-4160-ac3d-1f18de02dafd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.input_shape (32, 32, 3)\n",
      "X: (50000, 32, 32, 3), y: (50000, 1)\n",
      "X: (10000, 32, 32, 3), y: (10000, 1)\n",
      "[{'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}]\n",
      "********************\n",
      "#0, opt: RMSprop, lr: 0.001, loss_fn: CCE, batch: 800\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_31 (Conv2D)          (None, 32, 32, 16)        2368      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 32, 32, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPoolin  (None, 16, 16, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 16, 16, 32)        12832     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 16, 16, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 8, 8, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 2, 2, 256)         33024     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 146,154\n",
      "Trainable params: 144,650\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "2023-05-07 12:56:51.082428: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:56:51.082445: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:56:51.239145: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:56:51.247176: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 12:56:51.907081: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_16/dropout_31/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-05-07 12:56:53.739566: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 12:56:53.739585: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 12:56:53.926256: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 12:56:53.932470: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 12:56:53.948750: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 297 callback api events and 307 activity events. \n",
      "2023-05-07 12:56:53.951034: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 12:56:53.951219: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_12_56_53/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.14766, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 6s - loss: 2.2752 - accuracy: 0.2608 - val_loss: 2.1477 - val_accuracy: 0.2495 - 6s/epoch - 129ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 2.14766 to 2.11772, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 3s - loss: 1.8682 - accuracy: 0.3497 - val_loss: 2.1177 - val_accuracy: 0.2659 - 3s/epoch - 58ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 2.11772 to 2.10247, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 1.7181 - accuracy: 0.3963 - val_loss: 2.1025 - val_accuracy: 0.2463 - 2s/epoch - 52ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 2.10247 to 1.72357, saving model to ./cp/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 1.6268 - accuracy: 0.4339 - val_loss: 1.7236 - val_accuracy: 0.3671 - 2s/epoch - 57ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss improved from 1.72357 to 1.67407, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 1.5606 - accuracy: 0.4586 - val_loss: 1.6741 - val_accuracy: 0.3905 - 2s/epoch - 53ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss improved from 1.67407 to 1.32005, saving model to ./cp/cp-0006.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 1.4833 - accuracy: 0.4833 - val_loss: 1.3201 - val_accuracy: 0.5171 - 2s/epoch - 56ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss did not improve from 1.32005\n",
      "44/44 - 1s - loss: 1.4227 - accuracy: 0.5061 - val_loss: 1.4019 - val_accuracy: 0.5024 - 1s/epoch - 30ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss improved from 1.32005 to 1.23635, saving model to ./cp/cp-0008.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 1.3794 - accuracy: 0.5208 - val_loss: 1.2363 - val_accuracy: 0.5596 - 2s/epoch - 53ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.23635\n",
      "44/44 - 1s - loss: 1.3452 - accuracy: 0.5422 - val_loss: 1.4697 - val_accuracy: 0.4837 - 1s/epoch - 30ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.23635\n",
      "44/44 - 1s - loss: 1.2860 - accuracy: 0.5557 - val_loss: 1.5908 - val_accuracy: 0.4369 - 1s/epoch - 30ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss did not improve from 1.23635\n",
      "44/44 - 1s - loss: 1.2372 - accuracy: 0.5688 - val_loss: 1.9596 - val_accuracy: 0.3981 - 1s/epoch - 30ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.23635\n",
      "44/44 - 1s - loss: 1.1998 - accuracy: 0.5781 - val_loss: 1.4258 - val_accuracy: 0.4818 - 1s/epoch - 30ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.23635\n",
      "44/44 - 1s - loss: 1.1704 - accuracy: 0.5906 - val_loss: 1.3703 - val_accuracy: 0.5230 - 1s/epoch - 30ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss improved from 1.23635 to 1.20926, saving model to ./cp/cp-0014.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 1.1540 - accuracy: 0.5970 - val_loss: 1.2093 - val_accuracy: 0.5683 - 2s/epoch - 52ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss did not improve from 1.20926\n",
      "44/44 - 1s - loss: 1.1259 - accuracy: 0.6053 - val_loss: 1.6183 - val_accuracy: 0.4503 - 1s/epoch - 30ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.20926\n",
      "44/44 - 1s - loss: 1.0956 - accuracy: 0.6166 - val_loss: 1.4903 - val_accuracy: 0.4827 - 1s/epoch - 30ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 1.20926\n",
      "44/44 - 1s - loss: 1.0639 - accuracy: 0.6253 - val_loss: 2.3430 - val_accuracy: 0.2851 - 1s/epoch - 30ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss did not improve from 1.20926\n",
      "44/44 - 1s - loss: 1.0344 - accuracy: 0.6370 - val_loss: 1.2731 - val_accuracy: 0.5564 - 1s/epoch - 30ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss improved from 1.20926 to 1.08785, saving model to ./cp/cp-0019.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0019.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 3s - loss: 1.0103 - accuracy: 0.6423 - val_loss: 1.0879 - val_accuracy: 0.6227 - 3s/epoch - 58ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 1.08785\n",
      "44/44 - 1s - loss: 0.9893 - accuracy: 0.6486 - val_loss: 1.2293 - val_accuracy: 0.5689 - 1s/epoch - 30ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss improved from 1.08785 to 1.00017, saving model to ./cp/cp-0021.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0021.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0021.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 0.9634 - accuracy: 0.6563 - val_loss: 1.0002 - val_accuracy: 0.6424 - 2s/epoch - 53ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss did not improve from 1.00017\n",
      "44/44 - 1s - loss: 0.9477 - accuracy: 0.6637 - val_loss: 1.2425 - val_accuracy: 0.5575 - 1s/epoch - 31ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss did not improve from 1.00017\n",
      "44/44 - 1s - loss: 0.9440 - accuracy: 0.6625 - val_loss: 1.4081 - val_accuracy: 0.5507 - 1s/epoch - 30ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 1.00017\n",
      "44/44 - 1s - loss: 0.9167 - accuracy: 0.6761 - val_loss: 1.0445 - val_accuracy: 0.6367 - 1s/epoch - 30ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 1.00017\n",
      "44/44 - 1s - loss: 0.9007 - accuracy: 0.6785 - val_loss: 1.3472 - val_accuracy: 0.5434 - 1s/epoch - 29ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 1.00017\n",
      "44/44 - 1s - loss: 0.8914 - accuracy: 0.6883 - val_loss: 1.1139 - val_accuracy: 0.6096 - 1s/epoch - 30ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 1.00017\n",
      "44/44 - 1s - loss: 0.8773 - accuracy: 0.6914 - val_loss: 1.2091 - val_accuracy: 0.6117 - 1s/epoch - 31ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss improved from 1.00017 to 0.84484, saving model to ./cp/cp-0028.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0028.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 3s - loss: 0.8564 - accuracy: 0.6982 - val_loss: 0.8448 - val_accuracy: 0.7043 - 3s/epoch - 63ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.8486 - accuracy: 0.7014 - val_loss: 1.1127 - val_accuracy: 0.6140 - 1s/epoch - 31ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.8381 - accuracy: 0.7033 - val_loss: 0.8765 - val_accuracy: 0.6987 - 1s/epoch - 30ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.8218 - accuracy: 0.7091 - val_loss: 1.0466 - val_accuracy: 0.6451 - 1s/epoch - 30ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.8125 - accuracy: 0.7130 - val_loss: 1.8105 - val_accuracy: 0.4395 - 1s/epoch - 30ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.8039 - accuracy: 0.7165 - val_loss: 0.9026 - val_accuracy: 0.6895 - 1s/epoch - 30ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.7941 - accuracy: 0.7215 - val_loss: 1.4677 - val_accuracy: 0.5383 - 1s/epoch - 31ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.7898 - accuracy: 0.7221 - val_loss: 1.1383 - val_accuracy: 0.6177 - 1s/epoch - 30ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.84484\n",
      "44/44 - 1s - loss: 0.7752 - accuracy: 0.7271 - val_loss: 1.3859 - val_accuracy: 0.5473 - 1s/epoch - 30ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss improved from 0.84484 to 0.80481, saving model to ./cp/cp-0037.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0037.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0037.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 0.7637 - accuracy: 0.7322 - val_loss: 0.8048 - val_accuracy: 0.7245 - 2s/epoch - 57ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7625 - accuracy: 0.7310 - val_loss: 0.9706 - val_accuracy: 0.6635 - 1s/epoch - 30ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7529 - accuracy: 0.7336 - val_loss: 1.2476 - val_accuracy: 0.5923 - 1s/epoch - 29ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7485 - accuracy: 0.7372 - val_loss: 1.1241 - val_accuracy: 0.6362 - 1s/epoch - 29ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7357 - accuracy: 0.7426 - val_loss: 0.8156 - val_accuracy: 0.7158 - 1s/epoch - 30ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7329 - accuracy: 0.7405 - val_loss: 1.1532 - val_accuracy: 0.6314 - 1s/epoch - 29ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7143 - accuracy: 0.7479 - val_loss: 1.3325 - val_accuracy: 0.5869 - 1s/epoch - 30ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7189 - accuracy: 0.7444 - val_loss: 1.1861 - val_accuracy: 0.6335 - 1s/epoch - 29ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7060 - accuracy: 0.7515 - val_loss: 0.9970 - val_accuracy: 0.6667 - 1s/epoch - 30ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.80481\n",
      "44/44 - 1s - loss: 0.7096 - accuracy: 0.7508 - val_loss: 1.5692 - val_accuracy: 0.5634 - 1s/epoch - 29ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss improved from 0.80481 to 0.68505, saving model to ./cp/cp-0047.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0047.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0047.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 2s - loss: 0.7014 - accuracy: 0.7518 - val_loss: 0.6851 - val_accuracy: 0.7627 - 2s/epoch - 53ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.68505\n",
      "44/44 - 1s - loss: 0.6963 - accuracy: 0.7530 - val_loss: 1.1051 - val_accuracy: 0.6536 - 1s/epoch - 30ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.68505\n",
      "44/44 - 1s - loss: 0.6865 - accuracy: 0.7569 - val_loss: 0.9288 - val_accuracy: 0.6915 - 1s/epoch - 29ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.68505\n",
      "44/44 - 1s - loss: 0.6849 - accuracy: 0.7588 - val_loss: 0.8248 - val_accuracy: 0.7190 - 1s/epoch - 32ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.275150775909424, 1.8681690692901611, 1.7180752754211426, 1.6267753839492798, 1.560595989227295, 1.483339786529541, 1.4226967096328735, 1.37942636013031, 1.3451648950576782, 1.2860110998153687, 1.2372312545776367, 1.1998356580734253, 1.1704046726226807, 1.1540346145629883, 1.1259100437164307, 1.0955686569213867, 1.063921570777893, 1.034410834312439, 1.0102918148040771, 0.9892777800559998, 0.9634488821029663, 0.9476616978645325, 0.9439845085144043, 0.9167361259460449, 0.900661826133728, 0.8913763165473938, 0.8773336410522461, 0.8563756942749023, 0.8486010432243347, 0.8380791544914246, 0.8217993378639221, 0.8124951124191284, 0.8039334416389465, 0.7940708994865417, 0.7898371815681458, 0.7752440571784973, 0.7637273669242859, 0.7625163197517395, 0.7529481053352356, 0.7485321164131165, 0.7356886267662048, 0.7328625321388245, 0.7142506241798401, 0.7189481854438782, 0.7060105800628662, 0.709610641002655, 0.7013956308364868, 0.6963191032409668, 0.6864608526229858, 0.6849274635314941]\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6398 - accuracy: 0.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_31 (Conv2D)          (None, 32, 32, 16)        2368      \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 32, 32, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPoolin  (None, 16, 16, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 16, 16, 32)        12832     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 16, 16, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 8, 8, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_33 (Conv2D)          (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_34 (Conv2D)          (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 4, 4, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 2, 2, 256)         33024     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 2, 2, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " flatten_14 (Flatten)        (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 146,154\n",
      "Trainable params: 144,650\n",
      "Non-trainable params: 1,504\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'CCE', 'metrics': 'accuracy', 'batch': 800, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax', 'callbacks': [<keras.callbacks.ModelCheckpoint object at 0x7fc43ac49300>, <keras.callbacks.TensorBoard object at 0x7fc43cc58d00>]}\n",
      "MinMax\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "def main(benchmark):\n",
    "  train, test = benchmark.dataset()\n",
    "  X_train  = first(train)\n",
    "  y_train  = second(train)\n",
    "  X_test  = first(test)\n",
    "  y_test  = second(test)\n",
    "  print('X: {}, y: {}'.format(X_train.shape, y_train.shape))\n",
    "  print('X: {}, y: {}'.format(X_test.shape, y_test.shape))\n",
    "  result = benchmark.benchmark(X_train, y_train, X_test, y_test)\n",
    "\n",
    "benchmark = Cifar10Benchmark2()\n",
    "main(benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpqb5IwA3nrh"
   },
   "source": [
    "## Alexnet (3.7M Custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnJjLwC0qigZ"
   },
   "source": [
    "\n",
    "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png\" width=800 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90l81tYGWpXg"
   },
   "outputs": [],
   "source": [
    "class Cifar10Benchmark3(Cifar10Benchmark):\n",
    "    def __init__(self, name='Cifar', train_size=600):\n",
    "        super().__init__(name)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def dataset(self, shuffle=True, train_size=100):\n",
    "        train, test = cifar10.load_data()\n",
    "        X_train  = first(train)\n",
    "        #resized_X_train = benchmark.resize(X_train)\n",
    "        y_train  = second(train)\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        train_ds = (train_ds\n",
    "                .map(self.min_max)\n",
    "                .map(self.resize)\n",
    "        #        .map(self.to_categorical)\n",
    "                .shuffle(buffer_size=self.batch_size)\n",
    "                .batch(batch_size=self.batch_size, drop_remainder=True))\n",
    "        X_test  = first(test)\n",
    "        #resized_X_test = benchmark.resize(X_test)\n",
    "        y_test  = second(test)\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "        test_ds = (test_ds\n",
    "                .map(self.min_max)\n",
    "                .map(self.resize)\n",
    "        #        .map(self.to_categorical)\n",
    "                .shuffle(buffer_size=self.batch_size)\n",
    "                .batch(batch_size=self.batch_size, drop_remainder=True))\n",
    "        self.input_shape = train_ds.element_spec[1:]\n",
    "        self.train_ds_size = train_ds.cardinality().numpy()\n",
    "        #self.test_ds_size = test_ds.cardinality().numpy()\n",
    "        print('train_ds.element_spec', train_ds.element_spec) # (32, 227, 227, 3)\n",
    "        print('test_ds.element_spec', test_ds.element_spec) # (32, 227, 227, 3)\n",
    "        print('train_ds.cardinality', train_ds.cardinality().numpy()) \n",
    "        print('test_ds.cardinality', test_ds.cardinality().numpy()) \n",
    "        \n",
    "        return train_ds, test_ds\n",
    "\n",
    "    def make_hyper_params(self):\n",
    "        self.params = {\n",
    "            'opt': ['RMSprop'],\n",
    "            'lr': [0.001],\n",
    "            'loss_fn': ['SCCE'],\n",
    "            'metrics': ['accuracy'],\n",
    "            'validation_split': [0.3],\n",
    "            'verbose': [2],\n",
    "            'scaler': ['MinMax'],\n",
    "        }\n",
    "        import itertools\n",
    "        permutations_dicts = [dict(zip(self.params.keys(), v))  for v in itertools.product(*self.params.values())]\n",
    "        print(permutations_dicts)\n",
    "        return permutations_dicts\n",
    "\n",
    "    def to_categorical(self, X, y_label):\n",
    "        return X, tf.keras.utils.to_categorical(y_label)\n",
    "\n",
    "    def min_max(self, X, y_label):\n",
    "        return tf.image.per_image_standardization(X), y_label\n",
    "\n",
    "    def resize(self, X, y_label, height=227, width=227):\n",
    "        resized_X = tf.image.resize(X, (height, width))\n",
    "        return resized_X, y_label\n",
    "\n",
    "    def make_model(self, opt='Adam', lr='0.01', loss_fn='MSE', **kargs):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "        self.model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "\n",
    "        self.model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "\n",
    "        self.model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"))\n",
    "        self.model.add(BatchNormalization())\n",
    "        #model.add(MaxPool2D(pool_size=(3,3), strides=(2,2)))\n",
    "        self.model.add(GlobalMaxPool2D())\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(128, activation='relu'))\n",
    "        self.model.add(Dropout(0.5))\n",
    "        #model.add(Dense(4096, activation='relu'))\n",
    "        #model.add(Dropout(0.5))\n",
    "        self.model.add(Dense(10, activation='softmax'))\n",
    "        self.model.summary()\n",
    "\n",
    "        optimizer = self.make_optimizer(opt, lr)\n",
    "        loss_func = self.make_loss_func(loss_fn)\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_func, metrics=kargs['metrics'])\n",
    "        return self.model\n",
    "\n",
    "    def train(self, train_ds, epochs=50, batch=1, callbacks=None, **kargs):\n",
    "        train_size = int(self.train_ds_size*(1-kargs['validation_split']))\n",
    "        val_size = self.train_ds_size - train_size\n",
    "        splited_train_ds = train_ds.take(train_size)    \n",
    "        val_ds = train_ds.skip(train_size).take(val_size)\n",
    "        print('splited_train_ds.cardinality', splited_train_ds.cardinality().numpy()) \n",
    "        print('val_ds.cardinality', val_ds.cardinality().numpy())\n",
    "        return self.model.fit(\n",
    "            splited_train_ds, \n",
    "            epochs=epochs,\n",
    "            batch_size=self.batch_size, \n",
    "            callbacks=callbacks,\n",
    "            verbose=kargs['verbose'],\n",
    "            validation_data=val_ds,\n",
    "            )\n",
    "\n",
    "    def evaluate(self, X_ds):            \n",
    "        return self.model.evaluate(X_ds)\n",
    "\n",
    "    def predict(self, X_ds):\n",
    "        return self.model.predict(X_ds)\n",
    "\n",
    "    def benchmark(self, X_train_ds, X_test_ds, params=None):\n",
    "        import sys\n",
    "        hyper_params = self.make_hyper_params() if params is None else params\n",
    "        min_loss = sys.float_info.max\n",
    "        for i, param in enumerate(hyper_params):\n",
    "          print('*'*20)\n",
    "          print('#{}, opt: {}, lr: {}, loss_fn: {}, batch: {}'\\\n",
    "                .format(i, param['opt'], param['lr'], param['loss_fn'], self.batch_size))\n",
    "          self.make_model(**param)\n",
    "          param.update({\n",
    "              'callbacks': [self.make_checkpoint_callback(period=0),\n",
    "              self.make_tensorboard_callback()],\n",
    "              })\n",
    "          record = self.train(X_train_ds, **param)\n",
    "          result = self.parse_result(record)\n",
    "          print('loss', result['loss'])\n",
    "          if result['loss'][-1] < min_loss:      \n",
    "            score = self.evaluate(X_test_ds)\n",
    "            result['score'] = score\n",
    "            self.results.append(result)\n",
    "            self.best_param = param\n",
    "            if 'scaler' in param.keys():\n",
    "              self.best_scaler_type = param['scaler']\n",
    "            self.save_best_model()\n",
    "        self.load_best_model()\n",
    "        return self.results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tvb7HfIKaIW8",
    "outputId": "f7fca843-ad88-48d9-ea8e-2459674d741b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds.element_spec (TensorSpec(shape=(32, 227, 227, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))\n",
      "test_ds.element_spec (TensorSpec(shape=(32, 227, 227, 3), dtype=tf.float32, name=None), TensorSpec(shape=(32, 1), dtype=tf.uint8, name=None))\n",
      "train_ds.cardinality 1562\n",
      "test_ds.cardinality 312\n",
      "[{'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'SCCE', 'metrics': 'accuracy', 'batch': 32, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax'}]\n",
      "********************\n",
      "#0, opt: RMSprop, lr: 0.001, loss_fn: SCCE, batch: 32\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 55, 55, 96)       384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 27, 27, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,786,890\n",
      "Trainable params: 3,784,138\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splited_train_ds.cardinality 468\n",
      "val_ds.cardinality 1094\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 15:13:14.250458: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 15:13:14.250477: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 15:13:14.357788: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 15:13:14.359163: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 15:13:15.603784: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 15:13:15.603802: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 15:13:15.732760: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 15:13:15.734922: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 15:13:15.741684: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 187 callback api events and 197 activity events. \n",
      "2023-05-07 15:13:15.742991: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 15:13:15.743115: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_15_13_15/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.11479, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 25s - loss: 2.1845 - accuracy: 0.1898 - val_loss: 2.1148 - val_accuracy: 0.1898 - 25s/epoch - 53ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_loss improved from 2.11479 to 1.67051, saving model to ./cp/cp-0002.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0002.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.9139 - accuracy: 0.2738 - val_loss: 1.6705 - val_accuracy: 0.3750 - 23s/epoch - 50ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_loss improved from 1.67051 to 1.56194, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.7759 - accuracy: 0.3276 - val_loss: 1.5619 - val_accuracy: 0.4001 - 23s/epoch - 50ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_loss improved from 1.56194 to 1.46380, saving model to ./cp/cp-0004.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0004.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.6715 - accuracy: 0.3799 - val_loss: 1.4638 - val_accuracy: 0.4570 - 23s/epoch - 50ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_loss did not improve from 1.46380\n",
      "468/468 - 23s - loss: 1.5607 - accuracy: 0.4252 - val_loss: 1.5116 - val_accuracy: 0.4677 - 23s/epoch - 48ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_loss improved from 1.46380 to 1.36281, saving model to ./cp/cp-0006.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.4834 - accuracy: 0.4616 - val_loss: 1.3628 - val_accuracy: 0.5228 - 23s/epoch - 50ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_loss improved from 1.36281 to 1.30595, saving model to ./cp/cp-0007.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.4147 - accuracy: 0.4957 - val_loss: 1.3059 - val_accuracy: 0.5454 - 23s/epoch - 50ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_loss did not improve from 1.30595\n",
      "468/468 - 23s - loss: 1.3558 - accuracy: 0.5224 - val_loss: 1.4737 - val_accuracy: 0.5111 - 23s/epoch - 49ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_loss improved from 1.30595 to 1.05689, saving model to ./cp/cp-0009.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0009.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.2774 - accuracy: 0.5487 - val_loss: 1.0569 - val_accuracy: 0.6281 - 23s/epoch - 50ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.05689\n",
      "468/468 - 23s - loss: 1.2174 - accuracy: 0.5710 - val_loss: 1.1691 - val_accuracy: 0.5737 - 23s/epoch - 48ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_loss improved from 1.05689 to 1.01756, saving model to ./cp/cp-0011.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.1519 - accuracy: 0.5977 - val_loss: 1.0176 - val_accuracy: 0.6424 - 23s/epoch - 50ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.01756\n",
      "468/468 - 23s - loss: 1.0769 - accuracy: 0.6252 - val_loss: 1.1310 - val_accuracy: 0.6233 - 23s/epoch - 48ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_loss improved from 1.01756 to 1.00714, saving model to ./cp/cp-0013.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0013.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 1.0422 - accuracy: 0.6364 - val_loss: 1.0071 - val_accuracy: 0.6623 - 23s/epoch - 50ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_loss improved from 1.00714 to 0.97903, saving model to ./cp/cp-0014.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0014.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 0.9817 - accuracy: 0.6548 - val_loss: 0.9790 - val_accuracy: 0.6680 - 23s/epoch - 50ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_loss improved from 0.97903 to 0.97889, saving model to ./cp/cp-0015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 - 23s - loss: 0.9286 - accuracy: 0.6802 - val_loss: 0.9789 - val_accuracy: 0.6623 - 23s/epoch - 50ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.8671 - accuracy: 0.6984 - val_loss: 1.0684 - val_accuracy: 0.6584 - 23s/epoch - 48ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.8280 - accuracy: 0.7172 - val_loss: 1.1252 - val_accuracy: 0.6533 - 23s/epoch - 48ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.7669 - accuracy: 0.7368 - val_loss: 1.1534 - val_accuracy: 0.6600 - 23s/epoch - 48ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.97889\n",
      "468/468 - 22s - loss: 0.7379 - accuracy: 0.7491 - val_loss: 1.2055 - val_accuracy: 0.6518 - 22s/epoch - 48ms/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.97889\n",
      "468/468 - 22s - loss: 0.6774 - accuracy: 0.7666 - val_loss: 1.0678 - val_accuracy: 0.6793 - 22s/epoch - 48ms/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.6275 - accuracy: 0.7821 - val_loss: 1.0903 - val_accuracy: 0.6869 - 23s/epoch - 48ms/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.97889\n",
      "468/468 - 22s - loss: 0.5775 - accuracy: 0.7993 - val_loss: 1.1562 - val_accuracy: 0.6998 - 22s/epoch - 48ms/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.5335 - accuracy: 0.8170 - val_loss: 1.0219 - val_accuracy: 0.7158 - 23s/epoch - 48ms/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.4992 - accuracy: 0.8268 - val_loss: 1.3003 - val_accuracy: 0.6745 - 23s/epoch - 48ms/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.4787 - accuracy: 0.8394 - val_loss: 1.1639 - val_accuracy: 0.7094 - 23s/epoch - 48ms/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.4315 - accuracy: 0.8512 - val_loss: 1.1208 - val_accuracy: 0.7099 - 23s/epoch - 48ms/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.3981 - accuracy: 0.8660 - val_loss: 1.1194 - val_accuracy: 0.7246 - 23s/epoch - 48ms/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.3755 - accuracy: 0.8737 - val_loss: 1.2041 - val_accuracy: 0.7189 - 23s/epoch - 48ms/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.3588 - accuracy: 0.8780 - val_loss: 1.2600 - val_accuracy: 0.7133 - 23s/epoch - 48ms/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.3241 - accuracy: 0.8923 - val_loss: 1.2821 - val_accuracy: 0.7207 - 23s/epoch - 48ms/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.3011 - accuracy: 0.9015 - val_loss: 1.3408 - val_accuracy: 0.7219 - 23s/epoch - 48ms/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.2895 - accuracy: 0.9040 - val_loss: 1.3140 - val_accuracy: 0.7203 - 23s/epoch - 48ms/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.2828 - accuracy: 0.9114 - val_loss: 1.4829 - val_accuracy: 0.7274 - 23s/epoch - 48ms/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.97889\n",
      "468/468 - 22s - loss: 0.2552 - accuracy: 0.9169 - val_loss: 1.7517 - val_accuracy: 0.7136 - 22s/epoch - 48ms/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.2465 - accuracy: 0.9211 - val_loss: 1.6882 - val_accuracy: 0.7014 - 23s/epoch - 48ms/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.2436 - accuracy: 0.9217 - val_loss: 1.4764 - val_accuracy: 0.7370 - 23s/epoch - 48ms/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.97889\n",
      "468/468 - 22s - loss: 0.2288 - accuracy: 0.9298 - val_loss: 1.5704 - val_accuracy: 0.7265 - 22s/epoch - 48ms/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.2095 - accuracy: 0.9326 - val_loss: 1.6596 - val_accuracy: 0.7207 - 23s/epoch - 48ms/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.2025 - accuracy: 0.9375 - val_loss: 1.5855 - val_accuracy: 0.7324 - 23s/epoch - 48ms/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.97889\n",
      "468/468 - 22s - loss: 0.1993 - accuracy: 0.9407 - val_loss: 1.8815 - val_accuracy: 0.7140 - 22s/epoch - 48ms/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1848 - accuracy: 0.9436 - val_loss: 1.8879 - val_accuracy: 0.7193 - 23s/epoch - 48ms/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1694 - accuracy: 0.9470 - val_loss: 1.5766 - val_accuracy: 0.7354 - 23s/epoch - 48ms/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1745 - accuracy: 0.9468 - val_loss: 1.8589 - val_accuracy: 0.7157 - 23s/epoch - 48ms/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1721 - accuracy: 0.9493 - val_loss: 1.8579 - val_accuracy: 0.7319 - 23s/epoch - 48ms/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1490 - accuracy: 0.9534 - val_loss: 1.7052 - val_accuracy: 0.7334 - 23s/epoch - 48ms/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1482 - accuracy: 0.9561 - val_loss: 2.1874 - val_accuracy: 0.7094 - 23s/epoch - 48ms/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1516 - accuracy: 0.9537 - val_loss: 1.8590 - val_accuracy: 0.7182 - 23s/epoch - 48ms/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.97889\n",
      "468/468 - 22s - loss: 0.1508 - accuracy: 0.9568 - val_loss: 2.3096 - val_accuracy: 0.7210 - 22s/epoch - 48ms/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1329 - accuracy: 0.9600 - val_loss: 1.8747 - val_accuracy: 0.7377 - 23s/epoch - 48ms/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.97889\n",
      "468/468 - 23s - loss: 0.1317 - accuracy: 0.9604 - val_loss: 1.8096 - val_accuracy: 0.7472 - 23s/epoch - 48ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.1844756603240967, 1.9139456748962402, 1.7759029865264893, 1.6715357303619385, 1.560691475868225, 1.4834166765213013, 1.414701223373413, 1.3558375835418701, 1.277400255203247, 1.2174431085586548, 1.1518577337265015, 1.0769374370574951, 1.0422357320785522, 0.9817050099372864, 0.9286069273948669, 0.8671026229858398, 0.8279507160186768, 0.7668909430503845, 0.7378914952278137, 0.6774148941040039, 0.6275126934051514, 0.5775327682495117, 0.5335249304771423, 0.499159038066864, 0.47871336340904236, 0.43154239654541016, 0.3981161117553711, 0.3754943311214447, 0.35878244042396545, 0.32412171363830566, 0.3011363446712494, 0.28947213292121887, 0.2827703058719635, 0.25516003370285034, 0.2465135008096695, 0.24356384575366974, 0.22879114747047424, 0.2095201164484024, 0.20247788727283478, 0.19932210445404053, 0.18475112318992615, 0.16941678524017334, 0.17451877892017365, 0.17210973799228668, 0.1489952802658081, 0.14819025993347168, 0.15162554383277893, 0.15080423653125763, 0.1328631490468979, 0.13169173896312714]\n",
      "312/312 [==============================] - 3s 10ms/step - loss: 1.8913 - accuracy: 0.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 55, 55, 96)       384       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 27, 27, 96)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 27, 27, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 27, 27, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 13, 13, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 384)       885120    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 13, 13, 384)       1327488   \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 13, 13, 384)      1536      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 13, 13, 256)       884992    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 13, 13, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 256)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,786,890\n",
      "Trainable params: 3,784,138\n",
      "Non-trainable params: 2,752\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'SCCE', 'metrics': 'accuracy', 'batch': 32, 'validation_split': 0.3, 'verbose': 2, 'scaler': 'MinMax', 'callbacks': [<keras.callbacks.ModelCheckpoint object at 0x7fb440264b80>, <keras.callbacks.TensorBoard object at 0x7fb3ca5a21d0>]}\n",
      "MinMax\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "def main_alexnet(benchmark):\n",
    "  train_ds, test_ds = benchmark.dataset()\n",
    "  result = benchmark.benchmark(train_ds, test_ds)\n",
    "\n",
    "benchmark = Cifar10Benchmark3()\n",
    "main_alexnet(benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4OSqKyW4Dut"
   },
   "source": [
    "## VGG (3.9M Custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RWMPrw_rTYO"
   },
   "source": [
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Max-Ferguson/publication/322512435/figure/fig3/AS:697390994567179@1543282378794/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png\" width=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeTA-o8jbYGG"
   },
   "outputs": [],
   "source": [
    "class Cifar10BenchmarkVGG16(Cifar10Benchmark3):\n",
    "    def __init__(self, name='Cifar_VGG16', train_size=600):\n",
    "        super().__init__(name)\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def resize(self, X, y_label, height=224, width=224):\n",
    "        resized_X = tf.image.resize(X, (height, width))\n",
    "        return resized_X, y_label\n",
    "\n",
    "    def make_hyper_params(self):\n",
    "        self.params = {\n",
    "            'opt': ['RMSprop'],\n",
    "            'lr': [0.001],\n",
    "            'loss_fn': ['SCCE'],\n",
    "            'metrics': ['accuracy'],\n",
    "            'validation_split': [0.2],\n",
    "            'verbose': [2],\n",
    "            'scaler': ['MinMax'],\n",
    "            'epochs': [20],\n",
    "        }\n",
    "        import itertools\n",
    "        permutations_dicts = [dict(zip(self.params.keys(), v))  for v in itertools.product(*self.params.values())]\n",
    "        print(permutations_dicts)\n",
    "        return permutations_dicts\n",
    "\n",
    "    def make_model(self, opt='Adam', lr='0.01', loss_fn='MSE', **kargs):\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(Input(shape =(224,224,3)))\n",
    "\n",
    "        self.model.add(Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu'))\n",
    "        self.model.add(MaxPool2D(2,2))\n",
    "\n",
    "        self.model.add(Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu'))\n",
    "        self.model.add(MaxPool2D(2,2))\n",
    "\n",
    "        self.model.add(Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu'))\n",
    "        self.model.add(MaxPool2D(2,2))\n",
    "\n",
    "        self.model.add(Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'))\n",
    "        self.model.add(MaxPool2D(2, 2))\n",
    "\n",
    "        self.model.add(Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'))\n",
    "        #self.model.add(Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu'))\n",
    "        self.model.add(GlobalMaxPool2D())\n",
    "\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(units=128, activation='relu'))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(rate=0.5))\n",
    "        self.model.add(Dense(units=10, activation='softmax')) # 10 classes \n",
    "        self.model.summary()\n",
    "\n",
    "        optimizer = self.make_optimizer(opt, lr)\n",
    "        loss_func = self.make_loss_func(loss_fn)\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_func, metrics=kargs['metrics'])\n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_zoiYEwt5jP",
    "outputId": "eff2cf4b-38a2-4b63-f649-d315e9e5844b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds.element_spec (TensorSpec(shape=(64, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(64, 1), dtype=tf.uint8, name=None))\n",
      "test_ds.element_spec (TensorSpec(shape=(64, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(64, 1), dtype=tf.uint8, name=None))\n",
      "train_ds.cardinality 781\n",
      "test_ds.cardinality 156\n",
      "[{'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'SCCE', 'metrics': 'accuracy', 'validation_split': 0.2, 'verbose': 2, 'scaler': 'MinMax', 'epochs': 20}]\n",
      "********************\n",
      "#0, opt: RMSprop, lr: 0.001, loss_fn: SCCE, batch: 64\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_50 (Conv2D)          (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 112, 112, 64)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPoolin  (None, 56, 56, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_34 (MaxPoolin  (None, 28, 28, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " max_pooling2d_35 (MaxPoolin  (None, 14, 14, 512)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " global_max_pooling2d_8 (Glo  (None, 512)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,978,250\n",
      "Trainable params: 3,977,994\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splited_train_ds.cardinality 624\n",
      "val_ds.cardinality 157\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 22:48:25.598068: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 22:48:25.598089: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 22:48:25.697700: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 22:48:25.706919: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 22:48:26.643196: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-05-07 22:48:26.643216: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n",
      "2023-05-07 22:48:26.912458: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-05-07 22:48:26.919905: I tensorflow/core/profiler/backends/gpu/cupti_tracer.cc:1798] CUPTI activity buffer flushed\n",
      "2023-05-07 22:48:26.931374: I tensorflow/core/profiler/backends/gpu/cupti_collector.cc:522]  GpuTracer has collected 281 callback api events and 281 activity events. \n",
      "2023-05-07 22:48:26.933311: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-05-07 22:48:26.933494: I tensorflow/core/profiler/rpc/client/save_profile.cc:164] Collecting XSpace to repository: ./logs/plugins/profile/2023_05_07_22_48_26/edward-desktop.xplane.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 2.42324, saving model to ./cp/cp-0001.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0001.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 104s - loss: 2.2012 - accuracy: 0.1922 - val_loss: 2.4232 - val_accuracy: 0.1395 - 104s/epoch - 166ms/step\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 2: val_loss did not improve from 2.42324\n",
      "624/624 - 101s - loss: 1.9886 - accuracy: 0.2396 - val_loss: 5.5054 - val_accuracy: 0.1945 - 101s/epoch - 162ms/step\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: val_loss improved from 2.42324 to 1.86283, saving model to ./cp/cp-0003.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0003.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 102s - loss: 1.8091 - accuracy: 0.3144 - val_loss: 1.8628 - val_accuracy: 0.2429 - 102s/epoch - 164ms/step\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: val_loss did not improve from 1.86283\n",
      "624/624 - 102s - loss: 1.7277 - accuracy: 0.3321 - val_loss: 1.8907 - val_accuracy: 0.3303 - 102s/epoch - 163ms/step\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: val_loss improved from 1.86283 to 1.67465, saving model to ./cp/cp-0005.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0005.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 103s - loss: 1.7312 - accuracy: 0.3216 - val_loss: 1.6746 - val_accuracy: 0.3670 - 103s/epoch - 165ms/step\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: val_loss improved from 1.67465 to 1.37733, saving model to ./cp/cp-0006.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0006.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 101s - loss: 1.6553 - accuracy: 0.3513 - val_loss: 1.3773 - val_accuracy: 0.4880 - 101s/epoch - 163ms/step\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: val_loss improved from 1.37733 to 1.30086, saving model to ./cp/cp-0007.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0007.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 101s - loss: 1.5046 - accuracy: 0.4219 - val_loss: 1.3009 - val_accuracy: 0.5367 - 101s/epoch - 163ms/step\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 8: val_loss improved from 1.30086 to 1.23772, saving model to ./cp/cp-0008.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0008.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 102s - loss: 1.3248 - accuracy: 0.5123 - val_loss: 1.2377 - val_accuracy: 0.5266 - 102s/epoch - 163ms/step\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 9: val_loss did not improve from 1.23772\n",
      "624/624 - 99s - loss: 1.2509 - accuracy: 0.5388 - val_loss: 1.2638 - val_accuracy: 0.5366 - 99s/epoch - 158ms/step\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 10: val_loss did not improve from 1.23772\n",
      "624/624 - 100s - loss: 1.1686 - accuracy: 0.5708 - val_loss: 1.3833 - val_accuracy: 0.5619 - 100s/epoch - 160ms/step\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 11: val_loss improved from 1.23772 to 1.12304, saving model to ./cp/cp-0011.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0011.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 104s - loss: 1.1035 - accuracy: 0.5964 - val_loss: 1.1230 - val_accuracy: 0.6053 - 104s/epoch - 167ms/step\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 12: val_loss did not improve from 1.12304\n",
      "624/624 - 103s - loss: 1.0466 - accuracy: 0.6190 - val_loss: 113.3937 - val_accuracy: 0.4622 - 103s/epoch - 164ms/step\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 13: val_loss did not improve from 1.12304\n",
      "624/624 - 104s - loss: 1.0064 - accuracy: 0.6316 - val_loss: 1.3603 - val_accuracy: 0.5342 - 104s/epoch - 166ms/step\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 14: val_loss did not improve from 1.12304\n",
      "624/624 - 103s - loss: 0.9896 - accuracy: 0.6325 - val_loss: 1.1788 - val_accuracy: 0.6077 - 103s/epoch - 165ms/step\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 15: val_loss improved from 1.12304 to 1.00428, saving model to ./cp/cp-0015.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0015.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 105s - loss: 0.9463 - accuracy: 0.6495 - val_loss: 1.0043 - val_accuracy: 0.6393 - 105s/epoch - 169ms/step\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 16: val_loss did not improve from 1.00428\n",
      "624/624 - 105s - loss: 0.9283 - accuracy: 0.6479 - val_loss: 1.2039 - val_accuracy: 0.5817 - 105s/epoch - 168ms/step\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 17: val_loss improved from 1.00428 to 0.90509, saving model to ./cp/cp-0017.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0017.ckpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./cp/cp-0017.ckpt/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624/624 - 106s - loss: 0.8829 - accuracy: 0.6608 - val_loss: 0.9051 - val_accuracy: 0.6842 - 106s/epoch - 170ms/step\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.90509\n",
      "624/624 - 105s - loss: 0.8413 - accuracy: 0.6749 - val_loss: 1.3533 - val_accuracy: 0.5744 - 105s/epoch - 169ms/step\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.90509\n",
      "624/624 - 106s - loss: 0.8279 - accuracy: 0.6781 - val_loss: 2.0838 - val_accuracy: 0.4613 - 106s/epoch - 169ms/step\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.90509\n",
      "624/624 - 106s - loss: 0.8881 - accuracy: 0.6511 - val_loss: 1.0151 - val_accuracy: 0.6687 - 106s/epoch - 169ms/step\n",
      "parse_result:  dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "parsed_result:  dict_keys(['loss', 'weights', 'bias', 'accuracy'])\n",
      "loss [2.2012341022491455, 1.9886385202407837, 1.809080719947815, 1.7277215719223022, 1.7311919927597046, 1.6552670001983643, 1.5045769214630127, 1.324803352355957, 1.2508885860443115, 1.1685974597930908, 1.103485345840454, 1.0465590953826904, 1.0063774585723877, 0.9895524978637695, 0.9463159441947937, 0.9283159971237183, 0.8828586935997009, 0.8413377404212952, 0.8278685212135315, 0.8880637884140015]\n",
      "156/156 [==============================] - 7s 48ms/step - loss: 1.1338 - accuracy: 0.6645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_50 (Conv2D)          (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 112, 112, 64)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPoolin  (None, 56, 56, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_52 (Conv2D)          (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_34 (MaxPoolin  (None, 28, 28, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " max_pooling2d_35 (MaxPoolin  (None, 14, 14, 512)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " global_max_pooling2d_8 (Glo  (None, 512)              0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,978,250\n",
      "Trainable params: 3,977,994\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'opt': 'RMSprop', 'lr': 0.001, 'loss_fn': 'SCCE', 'metrics': 'accuracy', 'validation_split': 0.2, 'verbose': 2, 'scaler': 'MinMax', 'epochs': 20, 'callbacks': [<keras.callbacks.ModelCheckpoint object at 0x7fe2845103a0>, <keras.callbacks.TensorBoard object at 0x7fe1d821b070>]}\n",
      "MinMax\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "def main_vgg16(benchmark):\n",
    "  train_ds, test_ds = benchmark.dataset()\n",
    "  result = benchmark.benchmark(train_ds, test_ds)\n",
    "\n",
    "benchmark = Cifar10BenchmarkVGG16()\n",
    "main_vgg16(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8j9QATbzuxlh"
   },
   "outputs": [],
   "source": [
    "train_ds, test_ds = benchmark.dataset()\n",
    "params = benchmark.make_hyper_params()\n",
    "record = benchmark.train(train_ds, **params[0])\n",
    "result = benchmark.parse_result(record)\n",
    "print('loss', result['loss'])\n",
    "score = benchmark.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcAqv4C34LJW"
   },
   "source": [
    "## VGG16 (1.3B, 1000개)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JYsCH-Wd556"
   },
   "source": [
    "\n",
    "https://keras.io/api/applications/vgg/\n",
    "\n",
    "The default input size for this model is 224x224.\n",
    "\n",
    "Note: each Keras Application expects a specific kind of input preprocessing. For VGG16, call tf.keras.applications.vgg16.preprocess_input on your inputs before passing them to the model. vgg16.preprocess_input will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baWREV8kd5Mc",
    "outputId": "6118c452-dc20-406c-d07c-c7d482c3cd87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 23:48:53.134439: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-07 23:48:53.135029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-07 23:48:53.135170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-07 23:48:53.135231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-07 23:48:53.381244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-07 23:48:53.381377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-07 23:48:53.381439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-07 23:48:53.381490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9414 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-07 23:48:55.984866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-05-07 23:48:56.701276: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-07 23:49:02.448967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 115s - 115s/epoch - 147ms/step\n",
      "157/157 - 21s - 21s/epoch - 135ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers   import Input, Dense, Flatten\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, VGG16\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1: \n",
    "##gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "##tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "#2\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') # (50000, 32, 32, 3)\n",
    "x_test  = x_test.astype('float32')  # (10000, 32, 32, 3)\n",
    "\n",
    "# one-hot encoding \n",
    "#y_train = tf.keras.utils.to_categorical(y_train)\n",
    "#y_test  = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# preprocessing, 'caffe', x_train, x_test: BGR\n",
    "x_train = preprocess_input(x_train)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "#3: resize_layer\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "resize_layer = tf.keras.layers.Lambda(lambda img: tf.image.resize(img,(224, 224)))(inputs)\n",
    "\n",
    "#4:\n",
    "model = VGG16(\n",
    "    weights='imagenet', \n",
    "    include_top= True, \n",
    "    input_tensor= resize_layer  # input_tensor= inputs\n",
    "    )\n",
    "model.summary()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = test_ds.batch(64)\n",
    "\n",
    "train_result = model.predict(train_ds, verbose=2)\n",
    "test_result = model.predict(test_ds, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kz6yaaEXsvLv",
    "outputId": "b356e38e-ee78-4e25-bcdd-e2a7fb842c92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNM3gzh-4n0z"
   },
   "source": [
    "## VGG16 (66K, pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S9nDUNYBKXHH",
    "outputId": "3eadb55d-fa04-48d2-f30f-825faf484c24"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 00:04:50.037451: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 00:04:50.037929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 00:04:50.038083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 00:04:50.038145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 00:04:50.265301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 00:04:50.265425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 00:04:50.265489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 00:04:50.265540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9250 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,781,642\n",
      "Trainable params: 66,954\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 00:04:51.832623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-05-08 00:04:52.577188: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-08 00:04:58.288384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-08 00:04:58.299512: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x5622e3fa2aa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-08 00:04:58.299527: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2023-05-08 00:04:58.302597: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-08 00:04:58.334293: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-08 00:04:58.354611: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 - 124s - loss: 0.6709 - accuracy: 0.7747 - val_loss: 0.5059 - val_accuracy: 0.8314 - 124s/epoch - 226ms/step\n",
      "Epoch 2/10\n",
      "547/547 - 109s - loss: 0.4356 - accuracy: 0.8502 - val_loss: 0.5661 - val_accuracy: 0.8125 - 109s/epoch - 199ms/step\n",
      "Epoch 3/10\n",
      "547/547 - 108s - loss: 0.3723 - accuracy: 0.8723 - val_loss: 0.4574 - val_accuracy: 0.8473 - 108s/epoch - 198ms/step\n",
      "Epoch 4/10\n",
      "547/547 - 108s - loss: 0.3287 - accuracy: 0.8861 - val_loss: 0.4293 - val_accuracy: 0.8565 - 108s/epoch - 198ms/step\n",
      "Epoch 5/10\n",
      "547/547 - 105s - loss: 0.2923 - accuracy: 0.8982 - val_loss: 0.4546 - val_accuracy: 0.8518 - 105s/epoch - 192ms/step\n",
      "Epoch 6/10\n",
      "547/547 - 103s - loss: 0.2597 - accuracy: 0.9107 - val_loss: 0.5239 - val_accuracy: 0.8391 - 103s/epoch - 189ms/step\n",
      "Epoch 7/10\n",
      "547/547 - 105s - loss: 0.2329 - accuracy: 0.9214 - val_loss: 0.4818 - val_accuracy: 0.8566 - 105s/epoch - 192ms/step\n",
      "Epoch 8/10\n",
      "547/547 - 109s - loss: 0.2046 - accuracy: 0.9299 - val_loss: 0.5064 - val_accuracy: 0.8541 - 109s/epoch - 198ms/step\n",
      "Epoch 9/10\n",
      "547/547 - 108s - loss: 0.1842 - accuracy: 0.9361 - val_loss: 0.5045 - val_accuracy: 0.8555 - 108s/epoch - 198ms/step\n",
      "Epoch 10/10\n",
      "547/547 - 108s - loss: 0.1605 - accuracy: 0.9444 - val_loss: 0.5393 - val_accuracy: 0.8578 - 108s/epoch - 198ms/step\n",
      "1563/1563 - 114s - loss: 0.2510 - accuracy: 0.9274 - 114s/epoch - 73ms/step\n",
      "313/313 - 22s - loss: 0.5577 - accuracy: 0.8543 - 22s/epoch - 70ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers   import Input, Dense, Flatten\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input, VGG16\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1: \n",
    "##gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "##tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "#2\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') # (50000, 32, 32, 3)\n",
    "x_test  = x_test.astype('float32')  # (10000, 32, 32, 3)\n",
    "\n",
    "# one-hot encoding \n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test  = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# preprocessing, 'caffe', x_train, x_test: BGR\n",
    "x_train = preprocess_input(x_train)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "#3: resize_layer\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "resize_layer = tf.keras.layers.Lambda(lambda img: tf.image.resize(img,(224, 224)))(inputs) \n",
    "\n",
    "#4:\n",
    "vgg_model = VGG16(\n",
    "    weights='imagenet', \n",
    "    include_top= False, \n",
    "    input_tensor= resize_layer  # input_tensor= inputs\n",
    "    )\n",
    "\n",
    "vgg_model.trainable=False\n",
    "                      \n",
    "#4-1: output: classification\n",
    "x = vgg_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output  = Dense(10, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "model.summary()\n",
    "\n",
    "#5: train and evaluate the model\n",
    "#filepath = \"RES/ckpt/4404-model.h5\"\n",
    "#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath, verbose=0, save_best_only=True)\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "ret = model.fit(x_train, y_train, epochs=10, batch_size= 64, validation_split=0.3, verbose=2) #, callbacks = [cp_callback])\n",
    "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "#6: plot accuracy and loss\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "ax[0].plot(ret.history['loss'],  \"g-\")\n",
    "ax[0].set_title(\"train loss\")\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].set_ylim(0, 1.1)\n",
    "ax[1].plot(ret.history['accuracy'],     \"b-\", label=\"train accuracy\")\n",
    "ax[1].plot(ret.history['val_accuracy'], \"r-\", label=\"val accuracy\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsfrB2G95Hdx"
   },
   "source": [
    "# Step46 - ResNet Model\n",
    "\n",
    "https://paperswithcode.com/sota/image-classification-on-imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoJcnIov63t0"
   },
   "source": [
    "## ResNet50 (264K, pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WNt6T3qatboI",
    "outputId": "beaee21d-99f0-4bc1-dadc-b6125d256f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " lambda_2 (Lambda)              (None, 224, 224, 3)  0           ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 230, 230, 3)  0           ['lambda_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 112, 112, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 112, 112, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 112, 112, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 114, 114, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 56, 56, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 56, 56, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 56, 56, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 56, 56, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 56, 56, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 56, 56, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 56, 56, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 56, 56, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 56, 56, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 56, 56, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 56, 56, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 56, 56, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 28, 28, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 28, 28, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 28, 28, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 28, 28, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 28, 28, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 28, 28, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 28, 28, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 28, 28, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 28, 28, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 28, 28, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 28, 28, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 28, 28, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 14, 14, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 14, 14, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 14, 14, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 14, 14, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 14, 14, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 14, 14, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 14, 14, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 14, 14, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 14, 14, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 14, 14, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 14, 14, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 14, 14, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 14, 14, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 14, 14, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 7, 7, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 7, 7, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 7, 7, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 7, 7, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 7, 7, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 7, 7, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 7, 7, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 7, 7, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 7, 7, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 7, 7, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2 (Gl  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          262272      ['global_average_pooling2d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 10)           1290        ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,851,274\n",
      "Trainable params: 263,562\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "547/547 - 90s - loss: 0.4529 - accuracy: 0.8475 - val_loss: 0.3513 - val_accuracy: 0.8800 - 90s/epoch - 165ms/step\n",
      "Epoch 2/10\n",
      "547/547 - 84s - loss: 0.2849 - accuracy: 0.9029 - val_loss: 0.2956 - val_accuracy: 0.9003 - 84s/epoch - 154ms/step\n",
      "Epoch 3/10\n",
      "547/547 - 83s - loss: 0.2326 - accuracy: 0.9195 - val_loss: 0.3207 - val_accuracy: 0.8959 - 83s/epoch - 152ms/step\n",
      "Epoch 4/10\n",
      "547/547 - 82s - loss: 0.1937 - accuracy: 0.9315 - val_loss: 0.3129 - val_accuracy: 0.8974 - 82s/epoch - 150ms/step\n",
      "Epoch 5/10\n",
      "547/547 - 81s - loss: 0.1618 - accuracy: 0.9431 - val_loss: 0.3535 - val_accuracy: 0.8943 - 81s/epoch - 148ms/step\n",
      "Epoch 6/10\n",
      "547/547 - 81s - loss: 0.1356 - accuracy: 0.9508 - val_loss: 0.3365 - val_accuracy: 0.9007 - 81s/epoch - 149ms/step\n",
      "Epoch 7/10\n",
      "547/547 - 82s - loss: 0.1123 - accuracy: 0.9597 - val_loss: 0.3228 - val_accuracy: 0.9064 - 82s/epoch - 149ms/step\n",
      "Epoch 8/10\n",
      "547/547 - 79s - loss: 0.0921 - accuracy: 0.9664 - val_loss: 0.3565 - val_accuracy: 0.9039 - 79s/epoch - 145ms/step\n",
      "Epoch 9/10\n",
      "547/547 - 80s - loss: 0.0745 - accuracy: 0.9738 - val_loss: 0.3607 - val_accuracy: 0.9098 - 80s/epoch - 147ms/step\n",
      "Epoch 10/10\n",
      "547/547 - 83s - loss: 0.0645 - accuracy: 0.9767 - val_loss: 0.4032 - val_accuracy: 0.9023 - 83s/epoch - 151ms/step\n",
      "1563/1563 [==============================] - 84s 52ms/step\n",
      "confusion_matrix(C): tf.Tensor(\n",
      "[[4799    4   33    5   16    4    2    8  107   22]\n",
      " [   6 4842    1    2    2    0    0    1   26  120]\n",
      " [  18    0 4819   34   90   12   10    7    7    3]\n",
      " [   6    2   41 4684   64  166   15    9    5    8]\n",
      " [   6    0   41   18 4883   18    5   23    2    4]\n",
      " [   1    0   27  170   56 4719    3   23    0    1]\n",
      " [   8    2   92   74   85   24 4702    4    9    0]\n",
      " [   8    1   14   23  147   58    3 4741    1    4]\n",
      " [  15    6    5    2    6    1    0    0 4953   12]\n",
      " [   8   32    1    3    3    2    0    2   14 4935]], shape=(10, 10), dtype=int32)\n",
      "1563/1563 - 90s - loss: 0.1472 - accuracy: 0.9615 - 90s/epoch - 57ms/step\n",
      "313/313 - 18s - loss: 0.4258 - accuracy: 0.9048 - 18s/epoch - 59ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' \n",
    "ref1: https://keras.io/applications/\n",
    "ref2:https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet_common.py\n",
    "ref3:\n",
    "https://github.com/keras-team/keras-applications/releases/tag/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers   import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions \n",
    "from tensorflow.keras.preprocessing import image # pip install pillow\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1:\n",
    "##import tensorflow as tf\n",
    "##gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "##tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "\n",
    "#2\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')      # (50000, 32, 32, 3)\n",
    "x_test = x_test.astype('float32')        # (10000, 32, 32, 3)\n",
    "\n",
    "# one-hot encoding \n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# preprocessing, 'caffe', x_train, x_test: BGR\n",
    "x_train = preprocess_input(x_train)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "#3: resize_layer\n",
    "inputs = Input(shape = (32, 32, 3))\n",
    "resize_layer = tf.keras.layers.Lambda(lambda img: tf.image.resize(img,(224, 224)))(inputs)\n",
    "res_model = ResNet50(weights = 'imagenet', include_top = False, input_tensor = resize_layer)\n",
    "res_model.trainable=False\n",
    "\n",
    "#4: create top for cifar10 classification\n",
    "x = res_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "outs  = Dense(10, activation = 'softmax')(x)\n",
    "model = tf.keras.Model(inputs = inputs, outputs=outs)\n",
    "model.summary()\n",
    "\n",
    "#5: train and evaluate the model\n",
    "#filepath = \"RES/ckpt/4603-model.h5\"\n",
    "#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath, verbose = 0, save_best_only = True)\n",
    "                 \n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate = 0.001)\n",
    "model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "ret = model.fit(x_train, y_train, epochs = 10, batch_size = 64, validation_split = 0.3, verbose = 2) #, callbacks = [cp_callback])\n",
    "\n",
    "y_pred = model.predict(x_train)\n",
    "y_label = np.argmax(y_pred, axis = 1)\n",
    "C = tf.math.confusion_matrix(np.argmax(y_train, axis = 1), y_label)\n",
    "print(\"confusion_matrix(C):\", C)\n",
    "train_loss, train_acc = model.evaluate(x_train, y_train, verbose = 2)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose = 2)\n",
    "\n",
    "#6: plot accuracy and loss\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 6))\n",
    "ax[0].plot(ret.history['loss'], \"g-\")\n",
    "ax[0].set_title(\"train loss\")\n",
    "ax[0].set_xlabel('epochs')\n",
    "ax[0].set_ylabel('loss')\n",
    "\n",
    "ax[1].set_ylim(0, 1.1)\n",
    "ax[1].plot(ret.history['accuracy'], \"b-\", label = \"train accuracy\")\n",
    "ax[1].plot(ret.history['val_accuracy'], \"r-\", label = \"val_accuracy\")\n",
    "ax[1].set_title(\"accuracy\")\n",
    "ax[1].set_xlabel('epochs')\n",
    "ax[1].set_ylabel('accuracy')\n",
    "plt.legend(loc = 'lower right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVWiieGM7oV8"
   },
   "source": [
    "## Huggingface\n",
    "\n",
    "https://huggingface.co/docs/transformers/tasks/image_classification#image-classification\n",
    "\n",
    "\n",
    "https://huggingface.co/datasets/cifar10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RF9J4eyZ6po2",
    "outputId": "4bbe4125-47d3-4705-ee1e-1c02380a34e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Requirement already satisfied: requests in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Using cached pyarrow-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pandas in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from datasets) (2.0.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1 (from datasets)\n",
      "  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.0/154.0 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Collecting responses<0.19 (from datasets)\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: tokenizers, xxhash, tqdm, regex, pyarrow, fsspec, filelock, dill, responses, multiprocess, huggingface-hub, transformers, datasets, evaluate\n",
      "Successfully installed datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 filelock-3.12.0 fsspec-2023.4.0 huggingface-hub-0.14.1 multiprocess-0.70.14 pyarrow-12.0.0 regex-2023.5.5 responses-0.18.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.28.1 xxhash-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxPt5F9E_bYz",
    "outputId": "d3d9db40-1610-4503-870e-75ac0bafdbb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.0.6-py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipywidgets) (6.22.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipywidgets) (8.13.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.7 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.7-py3-none-any.whl (2.1 MB)\n",
      "Collecting jupyterlab-widgets~=3.0.7 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.7-py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.2.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.1)\n",
      "Requirement already satisfied: psutil in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.2)\n",
      "Requirement already satisfied: tornado>=6.1 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.1)\n",
      "Requirement already satisfied: backcall in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.5.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in ./miniconda3/envs/tf_gpu/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.0.6 jupyterlab-widgets-3.0.7 widgetsnbextension-4.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "3ad7bbc943ba4672b7aebc51074a92e0",
      "ecbacab31806433ea312c53e5e2dd634",
      "909b4fbf79b144efa7e0ceba46dd47b7",
      "0357700d63e64dde97a9e26a56ae3881",
      "1d78b82ccb994e99827215c04221471a",
      "00dec7f31f6e41b286ccbe26af40b8f9",
      "5ec2cefc00d2438f96bb0f80c553bed8",
      "b60b70feaa5447c09dac6b8dad27cf73",
      "4e23f595a052409a9038c28f1f0602ea",
      "4909e9069c10437689f5f7bc496ac3b0",
      "bccc6fb11d404f9bb215e8220126a5a3",
      "be205bcb04674ac0b77eda050cf231ba",
      "c46fbd8bf6c74dffbcb0e6cf3ded09a0",
      "b669267b4041409385d48a4e8f8f3759",
      "8c27d2cf145f429799bbad5d2035a76a",
      "e588f734cb9146c5acb7ed0a243d9926",
      "bb40a1642f064d30a7df8df7132aee71"
     ]
    },
    "id": "zG9Gy03m8Y4u",
    "outputId": "7e7503bb-4218-48ac-8b49-a87a2ab2ac53"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad7bbc943ba4672b7aebc51074a92e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25n90mVH8c3O",
    "outputId": "5e117a50-5ee8-4cd4-e877-39e3228b20de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cifar10'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "datasets_list = list_datasets()\n",
    "datasets_list[datasets_list.index('cifar10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "336331898fbc497a9d9d28b7b80928c2",
      "377e8e395f864542b5eb7ad2333963f4",
      "8f6c3b438719420aa4313d6be603cb22",
      "7fabbdd35ba34736ace478c078cde93b",
      "8e8f98c9a80149f3a7a6b8c9070d1ad7",
      "6446de00d19d4eacbfa5f593f4ec3784",
      "e7cc889fcb8f4d45abed8e763991647b",
      "02ec83ba98f44ddeab610d0f7dd5165a",
      "a9cf9c1adf3a463abc9935c342f62517",
      "adc793d6cf024f3c877442e03f7570b8",
      "293f40b1016e4f72aee3687e43320ae3"
     ]
    },
    "id": "Rgd5g_WB9SYV",
    "outputId": "d196ee58-1eb7-4692-c377-e20834269bd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (/home/edward/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "Found cached dataset cifar10 (/home/edward/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "Found cached dataset cifar10 (/home/edward/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "Found cached dataset cifar10 (/home/edward/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336331898fbc497a9d9d28b7b80928c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['img', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_train = load_dataset('cifar10', split='train')\n",
    "dataset_train = load_dataset('cifar10', split='test')\n",
    "dataset_train_5000 = load_dataset(\"cifar10\", split=\"train[:5000]\")\n",
    "dataset = load_dataset('cifar10')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U3ug6ZGU9VeO",
    "outputId": "b2a065f9-4250-4974-87e2-5a92c58812b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_dataset = dataset['train'].train_test_split(test_size=0.3)\n",
    "splitted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jF3vBWDWBwM-",
    "outputId": "0862c5cd-18c4-402c-c251-6abf872a9c9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n",
       " 'label': 3}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image: a PIL image of the food item\n",
    "# label: the label class of the food item\n",
    "splitted_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXOL1QaTBzqW",
    "outputId": "85567b9d-c8f1-4a2e-cf1b-c31e2119e4c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_dataset[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbnYm-tDCrhD"
   },
   "outputs": [],
   "source": [
    "labels = splitted_dataset[\"train\"].features[\"label\"].names\n",
    "label2id = {i:label for i, label in enumerate(labels)}\n",
    "id2label = {v:k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXoL5wR3DDoj",
    "outputId": "656ec1e4-791c-414b-ae0e-d862d411e93d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'airplane',\n",
       " 1: 'automobile',\n",
       " 2: 'bird',\n",
       " 3: 'cat',\n",
       " 4: 'deer',\n",
       " 5: 'dog',\n",
       " 6: 'frog',\n",
       " 7: 'horse',\n",
       " 8: 'ship',\n",
       " 9: 'truck'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mACcR2XCDFKV",
    "outputId": "9a6c3c6c-48a9-41de-c17e-afe9331f7819"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rnY4ENjDF_F"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QygeaQ5wDWmU",
    "outputId": "23b8ee65-699f-4036-99b5-69a7a1ea1b4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLLbiNi-LnRJ",
    "outputId": "c43400fe-6105-4b41-a7dd-a48fba5a6a4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 02:21:28.805253: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 02:21:28.805726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:21:28.805945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:21:28.806082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:21:29.055943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:21:29.056315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:21:29.056398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 02:21:29.056450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9108 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Resizing(size[0], size[1]),\n",
    "        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "val_data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Resizing(size[0], size[1]),\n",
    "        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name=\"val_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlX-zkC7DaBJ"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    # `expand_dims()` is used to add a batch dimension since\n",
    "    # the TF augmentation layers operates on batched inputs.\n",
    "    return tf.expand_dims(tf_image, 0)\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"img\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"img\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otnO_nERDlk3"
   },
   "outputs": [],
   "source": [
    "splitted_dataset[\"train\"].set_transform(preprocess_train)\n",
    "splitted_dataset[\"test\"].set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "df6zii_uDt2I"
   },
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sJ_tNgkDwIo",
    "outputId": "344e991c-a989-4735-ba85-32d56ce6856f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDcY3jLzD-HC",
    "outputId": "5aa95f0c-8c9a-427b-f8b8-a938bbbaaac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs: 5, batch_size: 16, num_train_steps: 175000, learning_rate: 3e-05\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_train_steps = len(splitted_dataset[\"train\"]) * num_epochs\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "print(f'num_epochs: {num_epochs}, batch_size: {batch_size}, num_train_steps: {num_train_steps}, learning_rate: {learning_rate}')\n",
    "\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJbBms8xEBgo",
    "outputId": "a1b07d69-95bb-4141-b0ce-a8df42da7978"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 02:21:39.797521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-05-08 02:21:40.536843: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-05-08 02:21:40.590692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  7690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,806,346\n",
      "Trainable params: 85,806,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jeb4yjmnEB-q",
    "outputId": "aa2b1cdb-f1fc-4da0-fdb8-06d314411de7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = splitted_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"pixel_values\"], \n",
    "    label_cols=[\"label\"], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = splitted_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=[\"pixel_values\"], \n",
    "    label_cols=[\"label\"], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Izs4_dS3G-eG",
    "outputId": "c7676b7d-ae92-474d-80d7-154edcc5e0e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_train_dataset.element_spec (TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n",
      "tf_eval_dataset.element_spec (TensorSpec(shape=(None, 3, 224, 224), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))\n",
      "tf_train_dataset.cardinality 2188\n",
      "tf_eval_dataset.cardinality 938\n"
     ]
    }
   ],
   "source": [
    "print('tf_train_dataset.element_spec', tf_train_dataset.element_spec) # (, 224, 224, 3)\n",
    "print('tf_eval_dataset.element_spec', tf_eval_dataset.element_spec) # (, 224, 224, 3)\n",
    "print('tf_train_dataset.cardinality', tf_train_dataset.cardinality().numpy()) \n",
    "print('tf_eval_dataset.cardinality', tf_eval_dataset.cardinality().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlrzvhNvIG9m"
   },
   "outputs": [],
   "source": [
    "# def min_max(X, y_label):\n",
    "#     return tf.image.per_image_standardization(X), y_label\n",
    "\n",
    "# def resize(X, y_label, height=224, width=224):\n",
    "#     resized_X = tf.image.resize(X, (height, width))\n",
    "#     return resized_X, y_label\n",
    "\n",
    "# X_train = splitted_dataset[\"train\"][\"pixel_values\"]\n",
    "# y_train = splitted_dataset[\"train\"].features[\"label\"]\n",
    "\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# train_ds = (train_ds\n",
    "#         .map(min_max)\n",
    "#         .map(resize)\n",
    "# #        .map(self.to_categorical)\n",
    "#         .shuffle(buffer_size=batch_size*4)\n",
    "#         .prefetch(tf.data.AUTOTUNE)\n",
    "#         .batch(batch_size=batch_size, drop_remainder=True))\n",
    "\n",
    "# X_val = splitted_dataset[\"test\"][\"pixel_values\"]\n",
    "# y_val = splitted_dataset[\"test\"].features[\"label\"]\n",
    "\n",
    "# val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "# val_ds = (val_ds\n",
    "#         .map(min_max)\n",
    "#         .map(resize)\n",
    "# #        .map(self.to_categorical)\n",
    "#         .shuffle(buffer_size=batch_size*4)\n",
    "#         .prefetch(tf.data.AUTOTUNE)\n",
    "#         .batch(batch_size=batch_size, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbVHRPySI6yU"
   },
   "outputs": [],
   "source": [
    "# print('train_ds.element_spec', train_ds.element_spec) # (, 224, 224, 3)\n",
    "# print('val_ds.element_spec', val_ds.element_spec) # (, 224, 224, 3)\n",
    "# print('tf_train_dataset.cardinality', tf_train_dataset.cardinality().numpy()) \n",
    "# print('val_ds.cardinality', val_ds.cardinality().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idFLUPwOETvt"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsJ-Gsp5EWFl"
   },
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n",
    "# push_to_hub_callback = PushToHubCallback(\n",
    "#     output_dir=\"cifar10_classifier\",\n",
    "#     tokenizer=image_processor,\n",
    "#     save_strategy=\"no\",\n",
    "# )\n",
    "callbacks = [metric_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3dNFbwTH5F1",
    "outputId": "1809e874-1768-4758-fc75-ea9560fc3573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2188/2188 [==============================] - 980s 445ms/step - loss: 0.4713 - val_loss: 0.1757 - accuracy: 0.9515\n",
      "Epoch 2/5\n",
      "2188/2188 [==============================] - 949s 434ms/step - loss: 0.0948 - val_loss: 0.1489 - accuracy: 0.9553\n",
      "Epoch 3/5\n",
      "2188/2188 [==============================] - 934s 427ms/step - loss: 0.0465 - val_loss: 0.1771 - accuracy: 0.9480\n",
      "Epoch 4/5\n",
      "2188/2188 [==============================] - 933s 426ms/step - loss: 0.0302 - val_loss: 0.1492 - accuracy: 0.9579\n",
      "Epoch 5/5\n",
      "2188/2188 [==============================] - 933s 427ms/step - loss: 0.0238 - val_loss: 0.1869 - accuracy: 0.9495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff28e4729b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evJCw3_EVAyo"
   },
   "outputs": [],
   "source": [
    "test_dataset = dataset['test']\n",
    "image = test_dataset[\"img\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTHKLkYfVAHk"
   },
   "outputs": [],
   "source": [
    "test_data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Resizing(size[0], size[1]),\n",
    "        tf.keras.layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name=\"test_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3otsQZfFVTgD"
   },
   "outputs": [],
   "source": [
    "def preprocess_test(example_batch):\n",
    "    \"\"\"Apply test_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        test_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"img\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xm6xIr_0EYgn",
    "outputId": "8d58d18b-8fe8-4512-f6b0-b8b4a1f75fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......rescaling\n",
      ".........vars\n",
      "......resizing\n",
      ".........vars\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-05-08 07:31:09          658\n",
      "metadata.json                                  2023-05-08 07:31:09           64\n",
      "variables.h5                                   2023-05-08 07:31:09         6336\n"
     ]
    }
   ],
   "source": [
    "test_dataset.set_transform(preprocess_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XzB0-gvTu79",
    "outputId": "906b3dea-bc71-41ed-afc8-22a6cb4bae16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tf_test_dataset = test_dataset.to_tf_dataset(\n",
    "    columns=[\"pixel_values\"], \n",
    "    label_cols=[\"label\"], \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size, \n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhPYgVRFVjlc",
    "outputId": "29939939-4614-4315-f6e3-1cb35de6bc67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188/2188 [==============================] - 292s 134ms/step - loss: 0.0359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.035935524851083755"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tf_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAWbMZrZVqWO",
    "outputId": "aef8e469-391b-4e39-d9eb-937fd99df9e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 82s 132ms/step - loss: 0.2065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20653016865253448"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XRSTIZ2FBFL"
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# classifier = pipeline(\"image-classification\", model=\"cifar10_classifier\")\n",
    "# classifier(image)\n",
    "# from transformers import AutoImageProcessor\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"YOUR_ID/cifar10_classifier\")\n",
    "# inputs = image_processor(image, return_tensors=\"tf\")\n",
    "# from transformers import TFAutoModelForImageClassification\n",
    "\n",
    "# model = TFAutoModelForImageClassification.from_pretrained(\"YOUR_ID/cifar10_classifier\")\n",
    "# logits = model(**inputs).logits\n",
    "# predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "# model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "63aNHdBdFEKc"
   },
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog10.png'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\n",
    "model = ViTForImageClassification.from_pretrained('nateraw/vit-base-patch16-224-cifar10')\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "classes = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "classes[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eecR9v6CFFsc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00dec7f31f6e41b286ccbe26af40b8f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_e588f734cb9146c5acb7ed0a243d9926",
      "placeholder": "​",
      "style": "IPY_MODEL_bb40a1642f064d30a7df8df7132aee71",
      "tabbable": null,
      "tooltip": null,
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "02ec83ba98f44ddeab610d0f7dd5165a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0357700d63e64dde97a9e26a56ae3881": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_allow_html": false,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_be205bcb04674ac0b77eda050cf231ba",
      "style": "IPY_MODEL_c46fbd8bf6c74dffbcb0e6cf3ded09a0",
      "tabbable": null,
      "tooltip": null,
      "value": true
     }
    },
    "1d78b82ccb994e99827215c04221471a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_b669267b4041409385d48a4e8f8f3759",
      "style": "IPY_MODEL_8c27d2cf145f429799bbad5d2035a76a",
      "tabbable": null,
      "tooltip": null
     }
    },
    "293f40b1016e4f72aee3687e43320ae3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "336331898fbc497a9d9d28b7b80928c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_377e8e395f864542b5eb7ad2333963f4",
       "IPY_MODEL_8f6c3b438719420aa4313d6be603cb22",
       "IPY_MODEL_7fabbdd35ba34736ace478c078cde93b"
      ],
      "layout": "IPY_MODEL_8e8f98c9a80149f3a7a6b8c9070d1ad7",
      "tabbable": null,
      "tooltip": null
     }
    },
    "377e8e395f864542b5eb7ad2333963f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_6446de00d19d4eacbfa5f593f4ec3784",
      "placeholder": "​",
      "style": "IPY_MODEL_e7cc889fcb8f4d45abed8e763991647b",
      "tabbable": null,
      "tooltip": null,
      "value": "100%"
     }
    },
    "3ad7bbc943ba4672b7aebc51074a92e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ecbacab31806433ea312c53e5e2dd634",
       "IPY_MODEL_909b4fbf79b144efa7e0ceba46dd47b7",
       "IPY_MODEL_0357700d63e64dde97a9e26a56ae3881",
       "IPY_MODEL_1d78b82ccb994e99827215c04221471a",
       "IPY_MODEL_00dec7f31f6e41b286ccbe26af40b8f9"
      ],
      "layout": "IPY_MODEL_5ec2cefc00d2438f96bb0f80c553bed8",
      "tabbable": null,
      "tooltip": null
     }
    },
    "4909e9069c10437689f5f7bc496ac3b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e23f595a052409a9038c28f1f0602ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "5ec2cefc00d2438f96bb0f80c553bed8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "6446de00d19d4eacbfa5f593f4ec3784": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fabbdd35ba34736ace478c078cde93b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_adc793d6cf024f3c877442e03f7570b8",
      "placeholder": "​",
      "style": "IPY_MODEL_293f40b1016e4f72aee3687e43320ae3",
      "tabbable": null,
      "tooltip": null,
      "value": " 2/2 [00:00&lt;00:00, 285.36it/s]"
     }
    },
    "8c27d2cf145f429799bbad5d2035a76a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_family": null,
      "font_size": null,
      "font_style": null,
      "font_variant": null,
      "font_weight": null,
      "text_color": null,
      "text_decoration": null
     }
    },
    "8e8f98c9a80149f3a7a6b8c9070d1ad7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f6c3b438719420aa4313d6be603cb22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_02ec83ba98f44ddeab610d0f7dd5165a",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a9cf9c1adf3a463abc9935c342f62517",
      "tabbable": null,
      "tooltip": null,
      "value": 2
     }
    },
    "909b4fbf79b144efa7e0ceba46dd47b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_allow_html": false,
      "disabled": false,
      "layout": "IPY_MODEL_4909e9069c10437689f5f7bc496ac3b0",
      "placeholder": "​",
      "style": "IPY_MODEL_bccc6fb11d404f9bb215e8220126a5a3",
      "tabbable": null,
      "tooltip": null,
      "value": ""
     }
    },
    "a9cf9c1adf3a463abc9935c342f62517": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "adc793d6cf024f3c877442e03f7570b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b60b70feaa5447c09dac6b8dad27cf73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b669267b4041409385d48a4e8f8f3759": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb40a1642f064d30a7df8df7132aee71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "bccc6fb11d404f9bb215e8220126a5a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "TextStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "TextStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "be205bcb04674ac0b77eda050cf231ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c46fbd8bf6c74dffbcb0e6cf3ded09a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "CheckboxStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "CheckboxStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": ""
     }
    },
    "e588f734cb9146c5acb7ed0a243d9926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7cc889fcb8f4d45abed8e763991647b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "ecbacab31806433ea312c53e5e2dd634": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_b60b70feaa5447c09dac6b8dad27cf73",
      "placeholder": "​",
      "style": "IPY_MODEL_4e23f595a052409a9038c28f1f0602ea",
      "tabbable": null,
      "tooltip": null,
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
